{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d56703b",
   "metadata": {},
   "source": [
    "# Predictive-Coding Style Diffusion on CIFAR-10\n",
    "\n",
    "This notebook implements a **predictive-coding-inspired** diffusion model:\n",
    "\n",
    "- Each 'layer' corresponds to a diffusion step.\n",
    "- The model predicts noise `ε` and reconstructs an estimate of the clean image `x₀`.\n",
    "- We add a predictive-coding auxiliary loss that penalizes the mismatch between the predicted noisy state `x̂_t` and the true `x_t`.\n",
    "- Training samples multiple timesteps per batch to emulate a stack of predictive layers.\n",
    "- Sampling uses a deterministic DDIM-like sampler.\n",
    "\n",
    "**Health checks** are embedded to confirm imports, CUDA, dataset wiring, and optional FID dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# torchvision\n",
    "try:\n",
    "    import torchvision\n",
    "    from torchvision import transforms\n",
    "    from torchvision.utils import make_grid, save_image\n",
    "    TV_OK = True\n",
    "except Exception as e:\n",
    "    TV_OK = False\n",
    "    print('torchvision import failed:', e)\n",
    "\n",
    "# tqdm\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    TQDM_OK = True\n",
    "except Exception as e:\n",
    "    TQDM_OK = False\n",
    "    print('tqdm import failed:', e)\n",
    "\n",
    "# Optional FID\n",
    "try:\n",
    "    from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "    TM_FID_OK = True\n",
    "except Exception as e:\n",
    "    TM_FID_OK = False\n",
    "    print('torchmetrics FID not available:', e)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "print('torch version:', torch.__version__)\n",
    "print('torchvision available:', TV_OK)\n",
    "print('tqdm available:', TQDM_OK)\n",
    "print('torchmetrics FID available:', TM_FID_OK)\n",
    "assert torch.__version__ >= '1.10', 'Please use torch>=1.10'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e029ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_root: str = './data'\n",
    "    image_size: int = 32\n",
    "    num_channels: int = 3\n",
    "    use_fake_data: bool = True\n",
    "    fake_data_len: int = 512\n",
    "\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 1\n",
    "    lr: float = 2e-4\n",
    "    grad_clip: float = 1.0\n",
    "    num_workers: int = 2\n",
    "    pc_layers_per_batch: int = 4\n",
    "    lambda_pc: float = 0.1\n",
    "    ema_decay: float = 0.999\n",
    "\n",
    "    timesteps: int = 1000\n",
    "    sampling_steps: int = 50\n",
    "    ddim_eta: float = 0.0\n",
    "\n",
    "    out_dir: str = './runs/pcn_diffusion_cifar'\n",
    "    save_every: int = 1\n",
    "    sample_grids: int = 64\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8692b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeCIFAR10(Dataset):\n",
    "    def __init__(self, n=512, img_size=32):\n",
    "        self.n = n\n",
    "        self.img_size = img_size\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.rand(3, self.img_size, self.img_size)*2-1\n",
    "        return x, 0\n",
    "\n",
    "def get_dataloaders(cfg: Config):\n",
    "    if cfg.use_fake_data or not TV_OK:\n",
    "        train_ds = FakeCIFAR10(cfg.fake_data_len, cfg.image_size)\n",
    "        test_ds  = FakeCIFAR10(cfg.fake_data_len//4, cfg.image_size)\n",
    "        print('[DATA] Using FakeCIFAR10 for smoke test.')\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(cfg.image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "        ])\n",
    "        train_ds = torchvision.datasets.CIFAR10(root=cfg.data_root, train=True, download=True, transform=transform)\n",
    "        test_ds  = torchvision.datasets.CIFAR10(root=cfg.data_root, train=False, download=True, transform=transform)\n",
    "        print('[DATA] Using real CIFAR-10.')\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = get_dataloaders(cfg)\n",
    "xb, yb = next(iter(train_loader))\n",
    "print('Batch:', xb.shape, xb.min().item(), xb.max().item())\n",
    "assert xb.shape[2] == cfg.image_size and xb.shape[3] == cfg.image_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d5e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    t = torch.linspace(0, T, T+1, dtype=torch.float32)\n",
    "    f = torch.cos(((t/T + s) / (1+s)) * math.pi / 2)**2\n",
    "    alphas_cumprod = f / f[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return betas.clamp(1e-8, 0.999)\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps: int):\n",
    "        betas = cosine_beta_schedule(timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), alphas_cumprod[:-1]], dim=0)\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.alphas_cumprod = alphas_cumprod\n",
    "        self.alphas_cumprod_prev = alphas_cumprod_prev\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n",
    "        self.one_over_sqrt_alphas = 1.0 / torch.sqrt(alphas)\n",
    "        self.posterior_mean_coef1 = (1 - alphas_cumprod_prev) / (1 - alphas_cumprod) * torch.sqrt(alphas)\n",
    "        self.posterior_mean_coef2 = (torch.sqrt(alphas_cumprod_prev) * betas) / (1 - alphas_cumprod)\n",
    "        self.posterior_variance = betas * (1 - alphas_cumprod_prev) / (1 - alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        sqrt_ac = self.sqrt_alphas_cumprod.to(x0.device)[t]\n",
    "        sqrt_om = self.sqrt_one_minus_alphas_cumprod.to(x0.device)[t]\n",
    "        while sqrt_ac.ndim < x0.ndim:\n",
    "            sqrt_ac = sqrt_ac[..., None]\n",
    "            sqrt_om = sqrt_om[..., None]\n",
    "        return sqrt_ac * x0 + sqrt_om * noise\n",
    "\n",
    "    def predict_x0_from_eps(self, x_t, t, eps):\n",
    "        sqrt_ac = self.sqrt_alphas_cumprod.to(x_t.device)[t]\n",
    "        sqrt_om = self.sqrt_one_minus_alphas_cumprod.to(x_t.device)[t]\n",
    "        while sqrt_ac.ndim < x_t.ndim:\n",
    "            sqrt_ac = sqrt_ac[..., None]\n",
    "            sqrt_om = sqrt_om[..., None]\n",
    "        return (x_t - sqrt_om * eps) / (sqrt_ac + 1e-8)\n",
    "\n",
    "    def predict_x_t_from_x0(self, x0, t, eps):\n",
    "        sqrt_ac = self.sqrt_alphas_cumprod.to(x0.device)[t]\n",
    "        sqrt_om = self.sqrt_one_minus_alphas_cumprod.to(x0.device)[t]\n",
    "        while sqrt_ac.ndim < x0.ndim:\n",
    "            sqrt_ac = sqrt_ac[..., None]\n",
    "            sqrt_om = sqrt_om[..., None]\n",
    "        return sqrt_ac * x0 + sqrt_om * eps\n",
    "\n",
    "diff = Diffusion(cfg.timesteps)\n",
    "print('Timesteps:', diff.timesteps, '| betas shape:', diff.betas.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ba35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        # fixed frequency range\n",
    "        freqs = torch.exp(torch.linspace(math.log(1.0), math.log(10000.0), steps=half, device=t.device))\n",
    "        t = t.float()[:, None]\n",
    "        emb = t * freqs[None, :]\n",
    "        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, t_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, out_ch))\n",
    "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    def forward(self, x, t_emb):\n",
    "        h = self.conv1(F.silu(self.norm1(x)))\n",
    "        t = self.time_mlp(t_emb)[:, :, None, None]\n",
    "        h = h + t\n",
    "        h = self.conv2(F.silu(self.norm2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, t_dim):\n",
    "        super().__init__()\n",
    "        self.block1 = ResBlock(in_ch, out_ch, t_dim)\n",
    "        self.block2 = ResBlock(out_ch, out_ch, t_dim)\n",
    "        self.down = nn.Conv2d(out_ch, out_ch, 4, stride=2, padding=1)\n",
    "    def forward(self, x, t_emb):\n",
    "        x = self.block1(x, t_emb)\n",
    "        x = self.block2(x, t_emb)\n",
    "        skip = x\n",
    "        x = self.down(x)\n",
    "        return x, skip\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, t_dim):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1)\n",
    "        self.block1 = ResBlock(out_ch*2, out_ch, t_dim)\n",
    "        self.block2 = ResBlock(out_ch, out_ch, t_dim)\n",
    "    def forward(self, x, skip, t_emb):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.block1(x, t_emb)\n",
    "        x = self.block2(x, t_emb)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, base=64, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.time_emb = SinusoidalTimeEmbedding(time_dim)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_dim, time_dim*4), nn.SiLU(), nn.Linear(time_dim*4, time_dim)\n",
    "        )\n",
    "        self.in_conv = nn.Conv2d(in_ch, base, 3, padding=1)\n",
    "        self.down1 = Down(base, base*2, time_dim)\n",
    "        self.down2 = Down(base*2, base*4, time_dim)\n",
    "        self.mid1 = ResBlock(base*4, base*4, time_dim)\n",
    "        self.mid2 = ResBlock(base*4, base*4, time_dim)\n",
    "        self.up2 = Up(base*4, base*2, time_dim)\n",
    "        self.up1 = Up(base*2, base, time_dim)\n",
    "        self.out_norm = nn.GroupNorm(8, base)\n",
    "        self.out_conv = nn.Conv2d(base, in_ch, 3, padding=1)\n",
    "    def forward(self, x, t):\n",
    "        t_emb = self.time_mlp(self.time_emb(t))\n",
    "        x = self.in_conv(x)\n",
    "        x, s1 = self.down1(x, t_emb)\n",
    "        x, s2 = self.down2(x, t_emb)\n",
    "        x = self.mid1(x, t_emb)\n",
    "        x = self.mid2(x, t_emb)\n",
    "        x = self.up2(x, s2, t_emb)\n",
    "        x = self.up1(x, s1, t_emb)\n",
    "        x = self.out_conv(F.silu(self.out_norm(x)))\n",
    "        return x\n",
    "\n",
    "net = UNet(in_ch=cfg.num_channels).to(device)\n",
    "print('UNet params (M):', sum(p.numel() for p in net.parameters())/1e6)\n",
    "xb_small = torch.randn(2, cfg.num_channels, cfg.image_size, cfg.image_size, device=device)\n",
    "tb = torch.randint(0, cfg.timesteps, (2,), device=device)\n",
    "with torch.no_grad():\n",
    "    out = net(xb_small, tb)\n",
    "print('UNet out:', out.shape)\n",
    "assert out.shape == xb_small.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea52b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.data.clone()\n",
    "    def update(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                new_avg = (1.0 - self.decay) * p.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_avg.clone()\n",
    "    def apply_to(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.data.copy_(self.shadow[name])\n",
    "\n",
    "def eps_mse_loss(pred_eps, eps):\n",
    "    return F.mse_loss(pred_eps, eps)\n",
    "\n",
    "def predictive_coding_aux_loss(x_t_hat, x_t):\n",
    "    return F.mse_loss(x_t_hat, x_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, ema, opt, loader, diff: Diffusion, cfg: Config, epoch: int):\n",
    "    model.train()\n",
    "    iterable = tqdm(loader, desc=f'Epoch {epoch}', leave=False) if TQDM_OK else loader\n",
    "    total_loss = 0.0\n",
    "    for batch in iterable:\n",
    "        x0, _ = batch\n",
    "        x0 = x0.to(device)\n",
    "        B = x0.size(0)\n",
    "        tset = torch.randint(0, diff.timesteps, (B, cfg.pc_layers_per_batch), device=device)\n",
    "        loss_eps_sum = 0.0\n",
    "        loss_pc_sum  = 0.0\n",
    "        for k in range(cfg.pc_layers_per_batch):\n",
    "            t = tset[:, k]\n",
    "            noise = torch.randn_like(x0)\n",
    "            x_t = diff.q_sample(x0, t, noise=noise)\n",
    "            pred_eps = model(x_t, t)\n",
    "            loss_eps = eps_mse_loss(pred_eps, noise)\n",
    "            x0_hat = diff.predict_x0_from_eps(x_t, t, pred_eps).clamp(-1, 1)\n",
    "            x_t_hat = diff.predict_x_t_from_x0(x0_hat, t, pred_eps.detach())\n",
    "            loss_pc = predictive_coding_aux_loss(x_t_hat, x_t)\n",
    "            loss_eps_sum = loss_eps_sum + loss_eps\n",
    "            loss_pc_sum  = loss_pc_sum  + loss_pc\n",
    "        loss = loss_eps_sum / cfg.pc_layers_per_batch + cfg.lambda_pc * (loss_pc_sum / cfg.pc_layers_per_batch)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        if cfg.grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "        ema.update(model)\n",
    "        total_loss += loss.item()\n",
    "        if TQDM_OK:\n",
    "            iterable.set_postfix(loss=f'{loss.item():.4f}')\n",
    "    return total_loss / max(1, len(loader))\n",
    "\n",
    "def save_samples_ddim(model, diff: Diffusion, cfg: Config, n: int = 64, fname: str = 'samples.png'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(n, cfg.num_channels, cfg.image_size, cfg.image_size, device=device)\n",
    "        t_seq = torch.linspace(diff.timesteps-1, 0, cfg.sampling_steps, device=device).long()\n",
    "        for i, t in enumerate(t_seq):\n",
    "            t_b = torch.full((n,), int(t.item()), device=device, dtype=torch.long)\n",
    "            eps = model(x, t_b)\n",
    "            a_bar_t = diff.sqrt_alphas_cumprod.to(device)[t_b]\n",
    "            while a_bar_t.ndim < x.ndim:\n",
    "                a_bar_t = a_bar_t[..., None]\n",
    "            if i == len(t_seq) - 1:\n",
    "                x = (x - (1 - a_bar_t**2).sqrt() * eps) / (a_bar_t + 1e-8)\n",
    "            else:\n",
    "                t_next = t_seq[i+1]\n",
    "                a_bar_next = diff.sqrt_alphas_cumprod.to(device)[t_next]\n",
    "                while a_bar_next.ndim < x.ndim:\n",
    "                    a_bar_next = a_bar_next[..., None]\n",
    "                x0_hat = (x - (1 - a_bar_t**2).sqrt() * eps) / (a_bar_t + 1e-8)\n",
    "                dir_xt = (1 - a_bar_next**2).sqrt() * eps\n",
    "                x = a_bar_next * x0_hat + dir_xt\n",
    "        x = x.clamp(-1, 1)\n",
    "        grid = make_grid((x+1)/2, nrow=int(math.sqrt(n)))\n",
    "        save_path = os.path.join(cfg.out_dir, fname)\n",
    "        save_image(grid, save_path)\n",
    "        return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c83b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(net.parameters(), lr=cfg.lr)\n",
    "ema = EMA(net, decay=cfg.ema_decay)\n",
    "steps_limit = 2 if cfg.use_fake_data else None\n",
    "net.train()\n",
    "epoch_loss = 0.0\n",
    "batches = 0\n",
    "it_ = tqdm(train_loader, desc='SmokeTrain', leave=False) if TQDM_OK else train_loader\n",
    "for i, batch in enumerate(it_):\n",
    "    if steps_limit is not None and i >= steps_limit:\n",
    "        break\n",
    "    loss = train_one_epoch(net, ema, opt, [batch], diff, cfg, epoch=0)\n",
    "    epoch_loss += loss\n",
    "    batches += 1\n",
    "    if TQDM_OK:\n",
    "        it_.set_postfix(avg_loss=f'{(epoch_loss/max(1,batches)):.4f}')\n",
    "print('Smoke training avg loss:', epoch_loss/max(1,batches))\n",
    "ema.apply_to(net)\n",
    "sample_path = save_samples_ddim(net, diff, cfg, n=min(cfg.sample_grids, 64), fname='samples_smoke.png')\n",
    "print('Saved sample grid to:', sample_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid_samples_vs_test(model, diff: Diffusion, cfg: Config, n_gen: int = 5000):\n",
    "    if not TM_FID_OK:\n",
    "        print('torchmetrics FID not available. Install torchmetrics to enable.')\n",
    "        return None\n",
    "    model.eval()\n",
    "    fid = FrechetInceptionDistance(feature=2048, reset_real_features=True).to(device)\n",
    "    count_real = 0\n",
    "    for xb, _ in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        xb_clamped = ((xb + 1)/2).clamp(0,1)\n",
    "        fid.update((xb_clamped*255).byte(), real=True)\n",
    "        count_real += xb.size(0)\n",
    "        if count_real >= n_gen:\n",
    "            break\n",
    "    gen_batch = cfg.batch_size\n",
    "    made = 0\n",
    "    while made < n_gen:\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(gen_batch, cfg.num_channels, cfg.image_size, cfg.image_size, device=device)\n",
    "            t_seq = torch.linspace(diff.timesteps-1, 0, cfg.sampling_steps, device=device).long()\n",
    "            for i, t in enumerate(t_seq):\n",
    "                t_b = torch.full((gen_batch,), int(t.item()), device=device, dtype=torch.long)\n",
    "                eps = model(x, t_b)\n",
    "                a_bar_t = diff.sqrt_alphas_cumprod.to(device)[t_b]\n",
    "                while a_bar_t.ndim < x.ndim:\n",
    "                    a_bar_t = a_bar_t[..., None]\n",
    "                if i == len(t_seq) - 1:\n",
    "                    x = (x - (1 - a_bar_t**2).sqrt() * eps) / (a_bar_t + 1e-8)\n",
    "                else:\n",
    "                    t_next = t_seq[i+1]\n",
    "                    a_bar_next = diff.sqrt_alphas_cumprod.to(device)[t_next]\n",
    "                    while a_bar_next.ndim < x.ndim:\n",
    "                        a_bar_next = a_bar_next[..., None]\n",
    "                    x0_hat = (x - (1 - a_bar_t**2).sqrt() * eps) / (a_bar_t + 1e-8)\n",
    "                    dir_xt = (1 - a_bar_next**2).sqrt() * eps\n",
    "                    x = a_bar_next * x0_hat + dir_xt\n",
    "            x = ((x.clamp(-1,1) + 1)/2).clamp(0,1)\n",
    "            fid.update((x*255).byte(), real=False)\n",
    "            made += gen_batch\n",
    "    score = float(fid.compute().cpu().item())\n",
    "    print(f'FID (approx, n_gen={n_gen}):', score)\n",
    "    return score\n",
    "print('FID function ready (may require internet to download Inception weights).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61e947",
   "metadata": {},
   "source": [
    "### Artifacts\n",
    "\n",
    "- A small sample grid was generated during the smoke test and saved in the run directory.\n",
    "- For full training on real CIFAR-10, set `use_fake_data=False` in the **Config** cell and run end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Notebook setup complete.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
