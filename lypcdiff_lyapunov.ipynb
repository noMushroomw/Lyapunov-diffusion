{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7461505c",
   "metadata": {},
   "source": [
    "# Lyapunov-Adaptive Predictive Coding Diffusion\n",
    "\n",
    "Unifies diffusion sampling with predictive coding and a Lyapunov-guided scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95ed6a",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "\n",
    "- Review configuration and data pipeline tuned for CIFAR-10.\n",
    "- Define the EDM-style predictive coding UNet backbone.\n",
    "- Formulate Lyapunov energy and the adaptive scheduler with optional policy head.\n",
    "- Implement the predictive-coding Heun sampler and training loop with AMP + EMA.\n",
    "- Provide hooks for sampling, checkpointing, and CleanFID evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ab2c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from cleanfid import fid as cleanfid\n",
    "except ImportError:\n",
    "    cleanfid = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28baa455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingConfig(data_root='./data', batch_size=256, num_workers=8, epochs=400, lr=0.0002, ema_decay=0.9999, sigma_data=0.5, sigma_min=0.002, sigma_max=80.0, steps=80, rho=7.0, grad_clip=1.0, amp=True, log_interval=50, sample_interval=5000, out_dir='runs/lyapunov_pc')\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    data_root: str = \"./data\"\n",
    "    batch_size: int = 256\n",
    "    num_workers: int = 8\n",
    "    epochs: int = 400\n",
    "    lr: float = 2e-4\n",
    "    ema_decay: float = 0.9999\n",
    "    sigma_data: float = 0.5\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80.0\n",
    "    steps: int = 80\n",
    "    rho: float = 7.0\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    log_interval: int = 50\n",
    "    sample_interval: int = 5000\n",
    "    out_dir: str = \"runs/lyapunov_pc\"\n",
    "\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "config = TrainingConfig()\n",
    "os.makedirs(config.out_dir, exist_ok=True)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3650e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.yixuan/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def build_dataloader(cfg: TrainingConfig) -> DataLoader:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.CIFAR10(root=cfg.data_root, train=True, download=True, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n",
    "    return loader\n",
    "\n",
    "train_loader = build_dataloader(config)\n",
    "print(f\"Training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e42d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, scale: float = 1.0):\n",
    "        super().__init__()\n",
    "        half = embedding_dim // 2\n",
    "        self.register_buffer(\"frequencies\", torch.exp(torch.linspace(math.log(1.0), math.log(1000.0), half)))\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, sigma: torch.Tensor) -> torch.Tensor:\n",
    "        sigma = sigma.view(-1, 1)\n",
    "        angles = sigma * self.frequencies[None] * self.scale\n",
    "        return torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
    "\n",
    "def zero_module(module: nn.Module) -> nn.Module:\n",
    "    for param in module.parameters():\n",
    "        nn.init.zeros_(param)\n",
    "    return module\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, emb_channels: int, dropout: float = 0.0, skip_rescale: bool = True):\n",
    "        super().__init__()\n",
    "        self.skip_rescale = skip_rescale\n",
    "        self.norm1 = nn.GroupNorm(num_groups=min(32, in_channels), num_channels=in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.emb_proj = nn.Linear(emb_channels, out_channels)\n",
    "        self.norm2 = nn.GroupNorm(num_groups=min(32, out_channels), num_channels=out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, temb: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.conv1(F.silu(self.norm1(x)))\n",
    "        h = h + self.emb_proj(F.silu(temb))[:, :, None, None]\n",
    "        h = self.conv2(self.dropout(F.silu(self.norm2(h))))\n",
    "        res = self.skip(x)\n",
    "        if self.skip_rescale:\n",
    "            return (res + h) / math.sqrt(2.0)\n",
    "        return res + h\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels: int, num_heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.norm = nn.GroupNorm(num_groups=min(32, channels), num_channels=channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1)\n",
    "        self.proj = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, h, w = x.shape\n",
    "        head_dim = c // self.num_heads\n",
    "        qkv = self.qkv(self.norm(x)).reshape(b, 3, self.num_heads, head_dim, h * w)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        attn = torch.einsum(\"bhci,bhcj->bhij\", q, k) * (head_dim ** -0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = torch.einsum(\"bhij,bhcj->bhci\", attn, v).reshape(b, c, h, w)\n",
    "        return x + self.proj(out)\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a60d873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 66,148,227\n"
     ]
    }
   ],
   "source": [
    "class EDMUNet(nn.Module):\n",
    "    def __init__(self, img_resolution: int = 32, in_channels: int = 3, out_channels: int = 3, model_channels: int = 192,\n",
    "                 channel_mult: Tuple[int, ...] = (1, 2, 2), num_res_blocks: int = 3, attn_resolutions: Tuple[int, ...] = (16,),\n",
    "                 dropout: float = 0.0, sigma_data: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        self.sigma_data = sigma_data\n",
    "        self.in_conv = nn.Conv2d(in_channels, model_channels, kernel_size=3, padding=1)\n",
    "        embed_dim = model_channels\n",
    "        self.fourier = FourierFeatures(embed_dim)\n",
    "        emb_channels = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(embed_dim, emb_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_channels, emb_channels)\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.downsamplers = nn.ModuleList()\n",
    "        self.skip_channels: List[int] = []\n",
    "        ch = model_channels\n",
    "        resolution = img_resolution\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            out_ch = model_channels * mult\n",
    "            block = nn.ModuleList()\n",
    "            for _ in range(num_res_blocks):\n",
    "                block.append(ResidualBlock(ch, out_ch, emb_channels, dropout))\n",
    "                ch = out_ch\n",
    "                if resolution in attn_resolutions:\n",
    "                    block.append(AttentionBlock(ch))\n",
    "            self.down_blocks.append(block)\n",
    "            self.skip_channels.append(ch)\n",
    "            if level < len(channel_mult) - 1:\n",
    "                self.downsamplers.append(Downsample(ch))\n",
    "                resolution //= 2\n",
    "            else:\n",
    "                self.downsamplers.append(nn.Identity())\n",
    "        self.mid_block = nn.ModuleList([\n",
    "            ResidualBlock(ch, ch, emb_channels, dropout),\n",
    "            AttentionBlock(ch),\n",
    "            ResidualBlock(ch, ch, emb_channels, dropout)\n",
    "        ])\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        for level, mult in reversed(list(enumerate(channel_mult))):\n",
    "            out_ch = model_channels * mult\n",
    "            block = nn.ModuleList()\n",
    "            for idx in range(num_res_blocks + 1):\n",
    "                skip_ch = self.skip_channels[level] if idx == 0 else 0\n",
    "                block.append(ResidualBlock(ch + skip_ch, out_ch, emb_channels, dropout))\n",
    "                ch = out_ch\n",
    "                if (img_resolution // (2 ** level)) in attn_resolutions:\n",
    "                    block.append(AttentionBlock(ch))\n",
    "            self.up_blocks.append(block)\n",
    "            if level > 0:\n",
    "                self.upsamplers.append(Upsample(ch))\n",
    "            else:\n",
    "                self.upsamplers.append(nn.Identity())\n",
    "        self.out_norm = nn.GroupNorm(num_groups=min(32, ch), num_channels=ch)\n",
    "        self.out_conv = zero_module(nn.Conv2d(ch, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c_noise: torch.Tensor) -> torch.Tensor:\n",
    "        sigma = torch.exp(4.0 * c_noise)\n",
    "        temb = self.time_embed(self.fourier(sigma))\n",
    "        h = self.in_conv(x)\n",
    "        residuals: List[torch.Tensor] = []\n",
    "        for block, downsample in zip(self.down_blocks, self.downsamplers):\n",
    "            for layer in block:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, temb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "            residuals.append(h)\n",
    "            h = downsample(h)\n",
    "        for layer in self.mid_block:\n",
    "            if isinstance(layer, ResidualBlock):\n",
    "                h = layer(h, temb)\n",
    "            else:\n",
    "                h = layer(h)\n",
    "        for block, upsample in zip(self.up_blocks, self.upsamplers):\n",
    "            skip = residuals.pop()\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            for layer in block:\n",
    "                if isinstance(layer, ResidualBlock):\n",
    "                    h = layer(h, temb)\n",
    "                else:\n",
    "                    h = layer(h)\n",
    "            h = upsample(h)\n",
    "        h = self.out_norm(h)\n",
    "        h = F.silu(h)\n",
    "        return self.out_conv(h)\n",
    "\n",
    "model = EDMUNet().to(device)\n",
    "ema = ExponentialMovingAverage(model.parameters(), decay=config.ema_decay)\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d494a641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule head: tensor([80.0000, 74.6325, 69.5766, 64.8172, 60.3397], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_karras_schedule(steps: int, sigma_min: float, sigma_max: float, rho: float, device: torch.device) -> torch.Tensor:\n",
    "    ramp = torch.linspace(0, 1, steps, device=device)\n",
    "    sigmas = (sigma_max ** (1 / rho) + ramp * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    return torch.cat([sigmas, sigmas.new_tensor([0.0])])\n",
    "\n",
    "def karras_sigma_distribution(batch_size: int, device: torch.device, p_mean: float = -1.2, p_std: float = 1.2) -> torch.Tensor:\n",
    "    return torch.exp(torch.randn(batch_size, device=device) * p_std + p_mean)\n",
    "\n",
    "def edm_preconditioning(sigma: torch.Tensor, sigma_data: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    sigma = sigma.view(-1, 1, 1, 1)\n",
    "    sigma2 = sigma.square()\n",
    "    sigma_data2 = sigma_data ** 2\n",
    "    c_skip = sigma_data2 / (sigma2 + sigma_data2)\n",
    "    c_out = sigma * sigma_data / torch.sqrt(sigma2 + sigma_data2)\n",
    "    c_in = 1.0 / torch.sqrt(sigma2 + sigma_data2)\n",
    "    c_noise = torch.log(sigma.squeeze(-1).squeeze(-1)) / 4.0\n",
    "    return c_in, c_out, c_skip, c_noise\n",
    "\n",
    "def predictive_coding_corrector(model: nn.Module, x_sigma: torch.Tensor, sigma: torch.Tensor, sigma_data: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    c_in, c_out, c_skip, c_noise = edm_preconditioning(sigma.to(x_sigma.device, x_sigma.dtype), sigma_data)\n",
    "    prediction = model(c_in * x_sigma, c_noise)\n",
    "    corrected = c_skip.view(-1, 1, 1, 1) * x_sigma + c_out.view(-1, 1, 1, 1) * prediction\n",
    "    residual = prediction - x_sigma\n",
    "    return corrected, residual, prediction\n",
    "\n",
    "@torch.no_grad()\n",
    "def edm_wrapper(model: nn.Module, x: torch.Tensor, sigma: torch.Tensor, sigma_data: float) -> torch.Tensor:\n",
    "    corrected, _, _ = predictive_coding_corrector(model, x, sigma, sigma_data)\n",
    "    return corrected\n",
    "\n",
    "sigma_schedule = get_karras_schedule(config.steps, config.sigma_min, config.sigma_max, config.rho, device)\n",
    "print(f\"Schedule head: {sigma_schedule[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a026ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/17960093/ipykernel_1707347/228489550.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=config.amp and device.type == \"cuda\")\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, betas=(0.9, 0.999), weight_decay=0.0)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp and device.type == \"cuda\")\n",
    "\n",
    "def predictive_coding_flow_loss(model: nn.Module, x0: torch.Tensor, cfg: TrainingConfig) -> Tuple[torch.Tensor, dict]:\n",
    "    sigma = karras_sigma_distribution(x0.shape[0], x0.device)\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_sigma = x0 + sigma.view(-1, 1, 1, 1) * noise\n",
    "    corrected, residual, prediction = predictive_coding_corrector(model, x_sigma, sigma, cfg.sigma_data)\n",
    "    loss = F.mse_loss(corrected, x0)\n",
    "    metrics = {\n",
    "        \"sigma_mean\": float(sigma.mean().item()),\n",
    "        \"residual_rms\": float(residual.pow(2).mean().sqrt().item())\n",
    "    }\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62103b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepController(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.tanh(self.net(features))\n",
    "\n",
    "class LyapunovAdaptiveScheduler:\n",
    "    def __init__(self, base_sigmas: torch.Tensor, sigma_data: float, alpha: float = 0.3,\n",
    "                 min_scale: float = 0.5, max_scale: float = 1.6, controller: Optional[StepController] = None):\n",
    "        self.base_sigmas = base_sigmas\n",
    "        self.sigma_data = sigma_data\n",
    "        self.alpha = alpha\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.device = base_sigmas.device\n",
    "        self.dtype = base_sigmas.dtype\n",
    "        self.controller = controller.to(self.device) if controller is not None else None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.index = 0\n",
    "        self.prev_energy: Optional[torch.Tensor] = None\n",
    "\n",
    "    def energy(self, x: torch.Tensor, sigma_value: torch.Tensor) -> torch.Tensor:\n",
    "        sigma_batch = sigma_value.view(1).to(x.device, x.dtype).expand(x.shape[0])\n",
    "        recon_term = (x / self.sigma_data).pow(2).mean(dim=(1, 2, 3))\n",
    "        return 0.5 * recon_term + torch.log(sigma_batch + 1e-8)\n",
    "\n",
    "    def scale_factor(self, drop: torch.Tensor, features: Optional[torch.Tensor]) -> float:\n",
    "        if self.controller is not None and features is not None:\n",
    "            features = features.to(self.device, dtype=self.dtype)\n",
    "            raw = self.controller(features).mean()\n",
    "            return float(torch.clamp(1.0 + raw, self.min_scale, self.max_scale))\n",
    "        return float(torch.clamp(1.0 + self.alpha * drop.mean(), self.min_scale, self.max_scale))\n",
    "\n",
    "    def current_sigma(self) -> torch.Tensor:\n",
    "        return self.base_sigmas[self.index]\n",
    "\n",
    "    def step(self, denoised: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        sigma_curr = self.base_sigmas[self.index]\n",
    "        if self.index >= len(self.base_sigmas) - 1:\n",
    "            return sigma_curr, self.base_sigmas[-1]\n",
    "        base_next = self.base_sigmas[self.index + 1]\n",
    "        energy_curr = self.energy(denoised, sigma_curr)\n",
    "        if self.prev_energy is None:\n",
    "            drop = torch.zeros_like(energy_curr)\n",
    "        else:\n",
    "            drop = self.prev_energy - energy_curr\n",
    "        features = None\n",
    "        if self.controller is not None:\n",
    "            features = torch.stack([\n",
    "                energy_curr.mean(),\n",
    "                drop.mean(),\n",
    "                torch.log(sigma_curr.clamp_min(1e-8))\n",
    "            ], dim=0).view(1, -1)\n",
    "        scale = self.scale_factor(drop, features)\n",
    "        sigma_next = sigma_curr + (base_next - sigma_curr) * scale\n",
    "        sigma_next = torch.clamp(sigma_next, min=float(self.base_sigmas[-1].item()), max=float(self.base_sigmas[0].item()))\n",
    "        sigma_next = torch.minimum(sigma_next, sigma_curr)\n",
    "        self.prev_energy = energy_curr.detach()\n",
    "        self.index += 1\n",
    "        return sigma_curr, sigma_next\n",
    "\n",
    "controller = StepController()\n",
    "scheduler = LyapunovAdaptiveScheduler(sigma_schedule, config.sigma_data, controller=controller)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f6e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predictive_coding_heun(model: nn.Module, scheduler: LyapunovAdaptiveScheduler, shape: Tuple[int, int, int, int],\n",
    "                           sigma_data: float, disable_tqdm: bool = False) -> torch.Tensor:\n",
    "    scheduler.reset()\n",
    "    sigmas = scheduler.base_sigmas\n",
    "    x = torch.randn(shape, device=device) * sigmas[0]\n",
    "    iterator = range(len(sigmas) - 1)\n",
    "    if not disable_tqdm:\n",
    "        iterator = tqdm(iterator, desc=\"Lyapunov PC Sampling\", leave=False)\n",
    "    for _ in iterator:\n",
    "        sigma_curr_tensor = scheduler.current_sigma()\n",
    "        sigma_batch = torch.full((shape[0],), float(sigma_curr_tensor.item()), device=device, dtype=x.dtype)\n",
    "        corrected, residual, _ = predictive_coding_corrector(model, x, sigma_batch, sigma_data)\n",
    "        sigma_curr_tensor, sigma_next_tensor = scheduler.step(corrected)\n",
    "        sigma_curr = float(sigma_curr_tensor.item())\n",
    "        sigma_next = float(sigma_next_tensor.item())\n",
    "        sigma_curr_batch = torch.full((shape[0],), sigma_curr, device=device, dtype=x.dtype)\n",
    "        corrected, residual, _ = predictive_coding_corrector(model, x, sigma_curr_batch, sigma_data)\n",
    "        d = (x - corrected) / sigma_curr_batch.view(-1, 1, 1, 1)\n",
    "        delta = sigma_next - sigma_curr\n",
    "        x_euler = x + delta * d\n",
    "        if abs(sigma_next) < 1e-12:\n",
    "            x = x_euler\n",
    "            break\n",
    "        sigma_next_batch = torch.full((shape[0],), sigma_next, device=device, dtype=x.dtype)\n",
    "        corrected_next, residual_next, _ = predictive_coding_corrector(model, x_euler, sigma_next_batch, sigma_data)\n",
    "        d_next = (x_euler - corrected_next) / sigma_next_batch.view(-1, 1, 1, 1)\n",
    "        x = x + 0.5 * delta * (d + d_next)\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_images(samples: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.clamp((samples + 1) / 2, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "304e3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "loss_history: List[float] = []\n",
    "\n",
    "def train(model: nn.Module, loader: DataLoader, cfg: TrainingConfig) -> None:\n",
    "    global global_step\n",
    "    model.train()\n",
    "    for epoch in range(cfg.epochs):\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch + 1}/{cfg.epochs}\")\n",
    "        for batch_idx, (images, _) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.amp and device.type == \"cuda\"):\n",
    "                loss, metrics = predictive_coding_flow_loss(model, images, cfg)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            ema.update()\n",
    "            loss_history.append(loss.item())\n",
    "            global_step += 1\n",
    "            if global_step % cfg.log_interval == 0:\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", rms=f\"{metrics['residual_rms']:.3f}\")\n",
    "            if global_step % cfg.sample_interval == 0:\n",
    "                with ema.average_parameters():\n",
    "                    samples = predictive_coding_heun(model, scheduler, (64, 3, 32, 32), cfg.sigma_data, disable_tqdm=True)\n",
    "                decoded = decode_images(samples)\n",
    "                save_path = os.path.join(cfg.out_dir, f\"samples_step_{global_step}.png\")\n",
    "                save_image(decoded, save_path, nrow=8)\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.out_dir, f\"model_epoch_{epoch + 1}.pth\"))\n",
    "        with ema.average_parameters():\n",
    "            torch.save(model.state_dict(), os.path.join(cfg.out_dir, f\"ema_epoch_{epoch + 1}.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7921c0c0",
   "metadata": {},
   "source": [
    "### Training Entry Point\n",
    "- Run the next cell to launch predictive-coding diffusion training with the Lyapunov adaptive scheduler.\n",
    "- The loop uses the forward predictive correction (flow matching) and updates EMA + sampling hooks automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_and_evaluation(cfg: TrainingConfig, loader: DataLoader, epochs_override: Optional[int] = None,\n",
    "                                  fid_total: int = 50000, fid_batch: int = 256) -> Optional[float]:\n",
    "    original_epochs = cfg.epochs\n",
    "    if epochs_override is not None:\n",
    "        cfg.epochs = epochs_override\n",
    "    try:\n",
    "        train(model, loader, cfg)\n",
    "    finally:\n",
    "        cfg.epochs = original_epochs\n",
    "    eval_scheduler = LyapunovAdaptiveScheduler(sigma_schedule, cfg.sigma_data, controller=controller)\n",
    "    with ema.average_parameters():\n",
    "        fid_score = evaluate_fid(model, eval_scheduler, cfg, total=fid_total, batch=fid_batch)\n",
    "    if fid_score is not None:\n",
    "        print(f\"Post-training FID (EMA): {fid_score:.3f}\")\n",
    "    return fid_score\n",
    "\n",
    "# Example usage:\n",
    "# fid = run_training_and_evaluation(config, train_loader, epochs_override=1, fid_total=1000, fid_batch=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbc1cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951c356d17324063b76837d5c4fc55cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lyapunov PC Sampling:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview saved to runs/lyapunov_pc/preview.png\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def sample_grid(model: nn.Module, scheduler: LyapunovAdaptiveScheduler, cfg: TrainingConfig, filename: str = \"preview.png\", batch: int = 64) -> str:\n",
    "    model.eval()\n",
    "    samples = predictive_coding_heun(model, scheduler, (batch, 3, 32, 32), cfg.sigma_data)\n",
    "    decoded = decode_images(samples)\n",
    "    out_path = os.path.join(cfg.out_dir, filename)\n",
    "    save_image(decoded, out_path, nrow=8)\n",
    "    return out_path\n",
    "\n",
    "preview_path = sample_grid(model, scheduler, config)\n",
    "print(f\"Preview saved to {preview_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "061d76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clean_fid(samples_dir: str, dataset: str = \"cifar10\", dataset_res: int = 32) -> Optional[float]:\n",
    "    if cleanfid is None:\n",
    "        print(\"CleanFID not installed. Install via `pip install clean-fid` to enable.\")\n",
    "        return None\n",
    "    return cleanfid.compute_fid(samples_dir, dataset_name=dataset, mode=\"clean\", dataset_res=dataset_res, dataset_split=\"train\")\n",
    "\n",
    "def evaluate_fid(model: nn.Module, scheduler: LyapunovAdaptiveScheduler, cfg: TrainingConfig, total: int = 50000, batch: int = 256) -> Optional[float]:\n",
    "    model.eval()\n",
    "    gen_dir = os.path.join(cfg.out_dir, \"generated\")\n",
    "    os.makedirs(gen_dir, exist_ok=True)\n",
    "    produced = 0\n",
    "    with torch.no_grad():\n",
    "        while produced < total:\n",
    "            current_batch = min(batch, total - produced)\n",
    "            samples = predictive_coding_heun(model, scheduler, (current_batch, 3, 32, 32), cfg.sigma_data, disable_tqdm=True)\n",
    "            decoded = (decode_images(samples) * 255).to(torch.uint8).cpu()\n",
    "            for idx in range(current_batch):\n",
    "                img = decoded[idx].permute(1, 2, 0).numpy()\n",
    "                Image.fromarray(img).save(os.path.join(gen_dir, f\"img_{produced + idx:05d}.png\"))\n",
    "            produced += current_batch\n",
    "            if produced % 1000 == 0:\n",
    "                print(f\"Generated {produced}/{total} images\")\n",
    "    fid_score = compute_clean_fid(gen_dir)\n",
    "    if fid_score is not None:\n",
    "        print(f\"FID: {fid_score:.3f}\")\n",
    "    return fid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47141463",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Train the controller with REINFORCE or GFlowNet-style objectives keyed on FID or Lyapunov drop.\n",
    "- Tune `alpha`, `steps`, and channel multipliers to align with the <2 FID target.\n",
    "- Integrate distributed training and mixed precision logging to reproduce EDM reference runs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
