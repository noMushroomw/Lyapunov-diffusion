{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676fa6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "Using device: cuda\n",
      "GPU Name: NVIDIA B200\n",
      "‚úÖ Imports successful.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Libraries & Setup Device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch_ema import ExponentialMovingAverage # A reliable EMA implementation\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "from cleanfid.fid import compute_fid\n",
    "import gdown # For downloading FID stats\n",
    "\n",
    "# --- Health Check ---\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"‚úÖ Imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69adc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining EDM model architecture...\n",
      "\n",
      "--- Health Check ---\n",
      "‚úÖ Model architecture defined successfully.\n",
      "Model classes: SongUNet, ResBlock, AttentionBlock, Upsample, Downsample\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define EDM Model Architecture\n",
    "print(\"Defining EDM model architecture...\")\n",
    "\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.max_positions = max_positions\n",
    "        self.endpoint = endpoint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure x is at least 1D\n",
    "        if x.ndim == 0:\n",
    "            x = x.unsqueeze(0)\n",
    "        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n",
    "        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n",
    "        freqs = (1 / self.max_positions) ** freqs\n",
    "        x = x.ger(freqs.to(x.dtype))\n",
    "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
    "        return x\n",
    "\n",
    "class SongUNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        label_dim           = 0,\n",
    "        augment_dim         = 0,\n",
    "        model_channels      = 128,\n",
    "        channel_mult        = [1, 2, 2, 2],\n",
    "        channel_mult_emb    = 4,\n",
    "        num_blocks          = 4,\n",
    "        attn_resolutions    = [16],\n",
    "        dropout             = 0.10,\n",
    "        resample_filter     = [1, 3, 3, 1],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        emb_channels = model_channels * channel_mult_emb\n",
    "        \n",
    "        # Noise level embedding\n",
    "        self.embed = torch.nn.Sequential(\n",
    "            PositionalEmbedding(num_channels=model_channels, max_positions=10000),\n",
    "            torch.nn.Linear(model_channels, emb_channels),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(emb_channels, emb_channels),\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_blocks = torch.nn.ModuleList()\n",
    "        self.downsamples = torch.nn.ModuleList()\n",
    "        channels_list = [model_channels * m for m in channel_mult]\n",
    "        in_ch = in_channels\n",
    "        current_res = img_resolution\n",
    "        \n",
    "        for level, out_ch in enumerate(channels_list):\n",
    "            level_blocks = torch.nn.ModuleList()\n",
    "            for block_idx in range(num_blocks):\n",
    "                level_blocks.append(ResBlock(in_channels=in_ch, out_channels=out_ch, \n",
    "                                            emb_channels=emb_channels, dropout=dropout))\n",
    "                in_ch = out_ch\n",
    "                \n",
    "                # Add attention if at specified resolution\n",
    "                if current_res in attn_resolutions:\n",
    "                    level_blocks.append(AttentionBlock(num_channels=out_ch))\n",
    "            \n",
    "            self.encoder_blocks.append(level_blocks)\n",
    "            \n",
    "            # Downsample (except at last level)\n",
    "            if level < len(channels_list) - 1:\n",
    "                self.downsamples.append(Downsample(in_channels=in_ch, out_channels=in_ch))\n",
    "                current_res //= 2\n",
    "            else:\n",
    "                self.downsamples.append(None)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_blocks = torch.nn.ModuleList()\n",
    "        self.upsamples = torch.nn.ModuleList()\n",
    "        \n",
    "        for level in reversed(range(len(channels_list))):\n",
    "            out_ch = channels_list[level]\n",
    "            level_blocks = torch.nn.ModuleList()\n",
    "            \n",
    "            # Upsample (except at last level which is actually first in decoder)\n",
    "            if level < len(channels_list) - 1:\n",
    "                self.upsamples.append(Upsample(in_channels=in_ch, out_channels=in_ch))\n",
    "                current_res *= 2\n",
    "            else:\n",
    "                self.upsamples.append(None)\n",
    "            \n",
    "            for block_idx in range(num_blocks):\n",
    "                level_blocks.append(ResBlock(in_channels=in_ch, out_channels=out_ch, \n",
    "                                            emb_channels=emb_channels, dropout=dropout))\n",
    "                in_ch = out_ch\n",
    "                \n",
    "                # Add attention if at specified resolution\n",
    "                if current_res in attn_resolutions:\n",
    "                    level_blocks.append(AttentionBlock(num_channels=out_ch))\n",
    "            \n",
    "            self.decoder_blocks.append(level_blocks)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.GroupNorm(num_groups=min(32, in_ch), num_channels=in_ch, eps=1e-5),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Conv2d(in_channels=in_ch, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c_noise, **model_kwargs):\n",
    "        emb = self.embed(c_noise)\n",
    "        \n",
    "        # Encoder\n",
    "        for level_blocks, downsample in zip(self.encoder_blocks, self.downsamples):\n",
    "            for block in level_blocks:\n",
    "                if isinstance(block, (ResBlock, AttentionBlock)):\n",
    "                    x, emb = block(x, emb)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "            if downsample is not None:\n",
    "                x = downsample(x)\n",
    "        \n",
    "        # Decoder\n",
    "        for upsample, level_blocks in zip(self.upsamples, self.decoder_blocks):\n",
    "            if upsample is not None:\n",
    "                x = upsample(x)\n",
    "            for block in level_blocks:\n",
    "                if isinstance(block, (ResBlock, AttentionBlock)):\n",
    "                    x, emb = block(x, emb)\n",
    "                else:\n",
    "                    x = block(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_channels, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = torch.nn.GroupNorm(num_groups=min(32, in_channels), num_channels=in_channels, eps=1e-5)\n",
    "        self.norm2 = torch.nn.GroupNorm(num_groups=min(32, out_channels), num_channels=out_channels, eps=1e-5)\n",
    "        self.emb_proj = torch.nn.Linear(emb_channels, out_channels * 2)\n",
    "        self.skip = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False) if in_channels != out_channels else torch.nn.Identity()\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, emb):\n",
    "        h = self.skip(x)\n",
    "        x = self.norm1(x)\n",
    "        x = torch.nn.functional.silu(x)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        emb_out = self.emb_proj(torch.nn.functional.silu(emb))\n",
    "        emb_out = emb_out[:, :, None, None]\n",
    "        scale, shift = emb_out.chunk(2, dim=1)\n",
    "        \n",
    "        x = self.norm2(x) * (1 + scale) + shift\n",
    "        x = torch.nn.functional.silu(x)\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x)\n",
    "        return x + h, emb\n",
    "\n",
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, num_channels, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.norm = torch.nn.GroupNorm(num_groups=min(32, num_channels), num_channels=num_channels, eps=1e-5)\n",
    "        self.qkv = torch.nn.Conv2d(num_channels, num_channels * 3, kernel_size=1)\n",
    "        self.proj = torch.nn.Conv2d(num_channels, num_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x, emb):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        qkv = qkv.reshape(b, 3, self.num_heads, c // self.num_heads, h * w)\n",
    "        qkv = qkv.permute(1, 0, 2, 4, 3)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "        out = out.permute(0, 1, 3, 2).reshape(b, c, h, w)\n",
    "        return x + self.proj(out), emb\n",
    "\n",
    "class Upsample(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Downsample(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7322601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Health Check ---\n",
      "Number of training samples: 50000\n",
      "Number of batches: 391\n",
      "Batch shape: torch.Size([128, 3, 32, 32])\n",
      "Batch data type: torch.float32\n",
      "Batch min value: -1.00\n",
      "Batch max value: 1.00\n",
      "‚úÖ Dataloader is working correctly.\n",
      "Batch shape: torch.Size([128, 3, 32, 32])\n",
      "Batch data type: torch.float32\n",
      "Batch min value: -1.00\n",
      "Batch max value: 1.00\n",
      "‚úÖ Dataloader is working correctly.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Dataloader (CIFAR-10)\n",
    "\n",
    "BATCH_SIZE = 128 # A B200 can handle a large batch size\n",
    "DATA_ROOT = './data'\n",
    "\n",
    "# Karras et al. normalize data to [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4, # You can increase this on a powerful machine\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# --- Health Check ---\n",
    "print(\"\\n--- Health Check ---\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of batches: {len(train_loader)}\")\n",
    "try:\n",
    "    x_batch, y_batch = next(iter(train_loader))\n",
    "    print(f\"Batch shape: {x_batch.shape}\")\n",
    "    print(f\"Batch data type: {x_batch.dtype}\")\n",
    "    print(f\"Batch min value: {x_batch.min():.2f}\")\n",
    "    print(f\"Batch max value: {x_batch.max():.2f}\")\n",
    "    assert x_batch.shape == (BATCH_SIZE, 3, 32, 32)\n",
    "    assert x_batch.min() >= -1.0 and x_batch.max() <= 1.0\n",
    "    print(\"‚úÖ Dataloader is working correctly.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataloader check failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a610b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Health Check ---\n",
      "‚úÖ Loss function health check passed.\n",
      "Test loss: 0.0620\n",
      "‚úÖ Loss function health check passed.\n",
      "Test loss: 0.0620\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: EDM Loss Function\n",
    "\n",
    "# Sigma distribution parameters from Karras et al. (2022)\n",
    "P_mean = -1.2\n",
    "P_std = 1.2\n",
    "\n",
    "def loss_fn(model, x_0):\n",
    "    \"\"\"\n",
    "    Implements the EDM denoising score matching loss.\n",
    "    (Matches slide image_9cd98a.png)\n",
    "    \"\"\"\n",
    "    # 1. \"Sample sigma ~ LogNormal(P_mean, P_std)\"\n",
    "    rnd_normal = torch.randn(x_0.shape[0], device=x_0.device)\n",
    "    sigma = (rnd_normal * P_std + P_mean).exp()\n",
    "    sigma = sigma.view(-1, 1, 1, 1) # Reshape for broadcasting\n",
    "    \n",
    "    # 2. \"draw n ~ N(0, I)\" (we'll multiply by sigma later)\n",
    "    n = torch.randn_like(x_0)\n",
    "    \n",
    "    # 3. \"set x_sigma = x_0 + sigma * n\"\n",
    "    x_sigma = x_0 + sigma * n\n",
    "    \n",
    "    # 4. \"optimize min E[ lambda(sigma) * ||D_theta(x_sigma) - x_0||^2 ]\"\n",
    "    # We need to compute D_theta(x_sigma) using the preconditioning.\n",
    "    \n",
    "    # EDM Preconditioning (from Karras et al.)\n",
    "    c_skip = 1.0 / (sigma ** 2 + 1.0)\n",
    "    c_out = sigma / (sigma ** 2 + 1.0).sqrt()\n",
    "    c_in = 1.0 / (sigma ** 2 + 1.0).sqrt()\n",
    "    c_noise = sigma.log() / 4\n",
    "    \n",
    "    # Run the U-Net\n",
    "    F_x = model(c_in * x_sigma, c_noise.squeeze())\n",
    "    \n",
    "    # This is the full denoiser D_theta(x_sigma)\n",
    "    D_theta = c_skip * x_sigma + c_out * F_x\n",
    "    \n",
    "    # The loss weight lambda(sigma). For the \"VP\" formulation, this is just 1.\n",
    "    # (As specified in EDM, Table 1)\n",
    "    weight = 1.0 \n",
    "    \n",
    "    # Final L2 loss\n",
    "    loss = weight * (D_theta - x_0) ** 2\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "# --- Health Check ---\n",
    "print(\"\\n--- Health Check ---\")\n",
    "try:\n",
    "    test_model = SongUNet(img_resolution=32, in_channels=3, out_channels=3).to(device)\n",
    "    test_batch = torch.randn(4, 3, 32, 32).to(device)\n",
    "    test_loss = loss_fn(test_model, test_batch)\n",
    "    print(f\"‚úÖ Loss function health check passed.\")\n",
    "    print(f\"Test loss: {test_loss.item():.4f}\")\n",
    "    assert test_loss.item() > 0\n",
    "    del test_model, test_batch, test_loss\n",
    "    torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Loss function check failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e44c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Health Check ---\n",
      "Model parameter count: 43,382,281\n",
      "‚úÖ Model, EMA, and Optimizer initialized.\n",
      "Starting training... This will take a while.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f746fe0c6e67428c8da7785555cc6131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Avg Loss: 0.1143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b17fe9c95884f53a15f70b5d015505a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete. Avg Loss: 0.0829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f91bea7b1994ff2b67df551b715a072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete. Avg Loss: 0.1006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89054bec38684b28be9fd5597efb183f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete. Avg Loss: 0.0854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe006a581b2492b9db6b2d7330d5c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete. Avg Loss: 0.0858\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df33b26fee56417893bf371295b2e003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 complete. Avg Loss: 0.0896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9db4b5a935640dfb5e2cd61db102f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 complete. Avg Loss: 0.0688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bae746f2b6461382c4034765f755a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 complete. Avg Loss: 0.0492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8297396798461986898f96dfb95022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/80:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: The Training Loop (Single GPU)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "EPOCHS = 80\n",
    "LEARNING_RATE = 2e-4\n",
    "EMA_DECAY = 0.999 # Karras et al. use an adaptive decay, but 0.999 is a good fixed value\n",
    "MODEL_CKPT_PATH = 'cifar10_model.pth'\n",
    "EMA_CKPT_PATH = 'cifar10_ema_model.pth'\n",
    "\n",
    "# 1. --- Initialize Model, EMA, and Optimizer ---\n",
    "model = SongUNet(\n",
    "    img_resolution=32,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    model_channels=128,\n",
    "    channel_mult=[1, 2, 2, 2],\n",
    "    attn_resolutions=[16],\n",
    "    num_blocks=4\n",
    ").to(device)\n",
    "\n",
    "# The EMA model is what we'll use for sampling.\n",
    "# It's a \"shadow\" copy of the model with smoother weights.\n",
    "ema = ExponentialMovingAverage(model.parameters(), decay=EMA_DECAY)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Health Check ---\n",
    "print(\"\\n--- Health Check ---\")\n",
    "print(f\"Model parameter count: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"‚úÖ Model, EMA, and Optimizer initialized.\")\n",
    "\n",
    "# 2. --- Start Training ---\n",
    "print(\"Starting training... This will take a while.\")\n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for x_batch, _ in progress_bar:\n",
    "        x_batch = x_batch.to(device)\n",
    "        \n",
    "        # --- Forward and Backward Pass ---\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(model, x_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Update EMA ---\n",
    "        # This is CRITICAL. We update the shadow weights.\n",
    "        ema.update()\n",
    "        \n",
    "        # --- Logging ---\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        step += 1\n",
    "        \n",
    "    # --- End of Epoch ---\n",
    "    print(f\"Epoch {epoch+1} complete. Avg Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    torch.save(model.state_dict(), MODEL_CKPT_PATH)\n",
    "    # To save the EMA model, we need to copy its parameters\n",
    "    with ema.average_parameters():\n",
    "        torch.save(model.state_dict(), EMA_CKPT_PATH)\n",
    "    \n",
    "    # We must restore the original weights after saving the EMA\n",
    "    # (The `torch_ema` library handles this with the context manager)\n",
    "\n",
    "print(\"‚úÖ Training complete.\")\n",
    "print(f\"Final model saved to: {MODEL_CKPT_PATH}\")\n",
    "print(f\"Final EMA model saved to: {EMA_CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: The \"Predictive Coding\" Sampler (Heun's 2nd Order)\n",
    "\n",
    "@torch.no_grad()\n",
    "def edm_wrapper(x, sigma, model):\n",
    "    \"\"\"\n",
    "    This is the D_theta(x, sigma) denoiser wrapper for *inference*.\n",
    "    It matches the preconditioning from the loss function.\n",
    "    \"\"\"\n",
    "    sigma = sigma.view(-1, 1, 1, 1) # Ensure correct shape\n",
    "    \n",
    "    c_skip = 1.0 / (sigma ** 2 + 1.0)\n",
    "    c_out = sigma / (sigma ** 2 + 1.0).sqrt()\n",
    "    c_in = 1.0 / (sigma ** 2 + 1.0).sqrt()\n",
    "    c_noise = sigma.log() / 4\n",
    "    \n",
    "    F_x = model(c_in * x, c_noise.squeeze())\n",
    "    D_theta = c_skip * x + c_out * F_x\n",
    "    return D_theta\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_pc_heun(\n",
    "    model,\n",
    "    shape,\n",
    "    sigmas,\n",
    "    device,\n",
    "    disable_tqdm=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements the \"Predictive Coding\" sampler (Heun's 2nd-order).\n",
    "    \"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Start from pure noise\n",
    "    x_k = torch.randn(shape, device=device) * sigmas[0]\n",
    "    \n",
    "    # Iterate through the K layers (noise levels)\n",
    "    for k in tqdm(range(len(sigmas) - 1), disable=disable_tqdm):\n",
    "        sigma_k = sigmas[k]\n",
    "        sigma_next = sigmas[k+1]\n",
    "        \n",
    "        # --- 1. First Evaluation (Predictive step) ---\n",
    "        # Get \"top-down prediction\" D_theta(x_k)\n",
    "        pred_x0 = edm_wrapper(x_k, torch.tensor([sigma_k], device=device), model)\n",
    "        \n",
    "        # Get \"bottom-up error unit\" e_k (the denoising residual)\n",
    "        # We use the \"score\" form (x - D(x))/sigma for stability\n",
    "        e_k = (x_k - pred_x0) / sigma_k\n",
    "        \n",
    "        # --- 2. Euler Step (Provisional Update) ---\n",
    "        x_provisional = x_k + (sigma_next - sigma_k) * e_k\n",
    "        \n",
    "        # --- 3. Corrector Step ---\n",
    "        if sigma_next == 0:\n",
    "            x_k = x_provisional\n",
    "            break\n",
    "        else:\n",
    "            pred_x0_next = edm_wrapper(x_provisional, torch.tensor([sigma_next], device=device), model)\n",
    "            e_next_corrected = (x_provisional - pred_x0_next) / sigma_next\n",
    "        \n",
    "        # --- 4. Final State Update (Averaging errors) ---\n",
    "        w_k_gain = (sigma_next - sigma_k)\n",
    "        x_k = x_k + (w_k_gain / 2.0) * (e_k + e_next_corrected)\n",
    "        \n",
    "    return x_k\n",
    "\n",
    "def get_karras_schedule(K=80, sigma_min=0.002, sigma_max=80.0, rho=7., device='cuda'):\n",
    "    \"\"\"Generates the Karras (EDM) noise schedule.\"\"\"\n",
    "    steps = torch.arange(K, device=device, dtype=torch.float32)\n",
    "    sigmas = (sigma_max**(1/rho) + steps/(K-1) * (sigma_min**(1/rho) - sigma_max**(1/rho)))**rho\n",
    "    sigmas = torch.cat([sigmas, torch.tensor([0.0], device=device)])\n",
    "    return sigmas\n",
    "\n",
    "# --- Health Check ---\n",
    "print(\"\\n--- Health Check ---\")\n",
    "print(\"‚úÖ Predictive Coding (Heun) Sampler and Schedule defined.\")\n",
    "try:\n",
    "    test_sched = get_karras_schedule(K=5, device=device)\n",
    "    print(f\"Test schedule (K=5): {test_sched.cpu().numpy()}\")\n",
    "    assert len(test_sched) == 6\n",
    "    print(\"‚úÖ Schedule logic seems correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Schedule logic error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22908f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate a Sample Grid\n",
    "\n",
    "print(\"Loading trained EMA model for sampling...\")\n",
    "\n",
    "# 1. --- Load the EMA Model ---\n",
    "# We instantiate a new model and load the EMA weights into it.\n",
    "eval_model = SongUNet(\n",
    "    img_resolution=32,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    model_channels=128,\n",
    "    channel_mult=[1, 2, 2, 2],\n",
    "    attn_resolutions=[16],\n",
    "    num_blocks=4\n",
    ").to(device)\n",
    "\n",
    "eval_model.load_state_dict(torch.load(EMA_CKPT_PATH, map_location=device))\n",
    "eval_model.eval()\n",
    "\n",
    "# 2. --- Settings for Sampling ---\n",
    "NUM_STEPS = 80  # K, number of \"PC layers\".\n",
    "GRID_SIZE = 64\n",
    "IMG_SHAPE = (GRID_SIZE, 3, 32, 32)\n",
    "\n",
    "# 3. --- Get Schedule & Sample ---\n",
    "sigmas = get_karras_schedule(K=NUM_STEPS, sigma_min=0.002, sigma_max=80.0, rho=7.0, device=device)\n",
    "print(f\"Running PC Sampler with {NUM_STEPS} steps...\")\n",
    "images = sample_pc_heun(\n",
    "    model=eval_model,\n",
    "    shape=IMG_SHAPE,\n",
    "    sigmas=sigmas,\n",
    "    device=device\n",
    ")\n",
    "print(\"...Sampling complete.\")\n",
    "\n",
    "# 4. --- Post-process and Save ---\n",
    "# Clamp to [-1, 1] and scale to [0, 255]\n",
    "images = (images.clamp(-1, 1) + 1) / 2\n",
    "images = (images * 255).to(torch.uint8)\n",
    "\n",
    "grid = make_grid(images, nrow=8)\n",
    "save_image(grid / 255.0, 'trained_pc_sampler_grid.png')\n",
    "\n",
    "# --- Health Check ---\n",
    "print(\"\\n--- Health Check ---\")\n",
    "print(f\"Output image tensor shape: {images.shape}\")\n",
    "print(f\"Output data type: {images.dtype}\")\n",
    "print(\"‚úÖ Saved 'trained_pc_sampler_grid.png'. Go check it out!\")\n",
    "\n",
    "# Display the image in the notebook\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(Image.open('trained_pc_sampler_grid.png'))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: FID Evaluation (Full)\n",
    "\n",
    "# --- Settings ---\n",
    "NUM_IMAGES_FOR_FID = 10000 # 50k is standard, 10k is a fast check\n",
    "FID_BATCH_SIZE = 128\n",
    "GEN_DIR = \"generated_images_scratch\"\n",
    "NUM_STEPS = 80  # K, number of \"PC layers\" for sampling\n",
    "\n",
    "if not os.path.exists(GEN_DIR):\n",
    "    os.makedirs(GEN_DIR)\n",
    "\n",
    "# --- Download FID stats ---\n",
    "stats_path = 'cifar10_train.npz'\n",
    "if not os.path.exists(stats_path):\n",
    "    print(\"Downloading CIFAR-10 FID stats...\")\n",
    "    gdown.download('https://github.com/GaParmar/clean-fid/raw/main/clean_fid/stats/cifar10_train.npz', stats_path, quiet=False)\n",
    "else:\n",
    "    print(\"FID stats file already exists.\")\n",
    "\n",
    "# --- Generate Images ---\n",
    "print(f\"Generating {NUM_IMAGES_FOR_FID} images for FID evaluation...\")\n",
    "num_generated = 0\n",
    "sigmas = get_karras_schedule(K=NUM_STEPS, sigma_min=0.002, sigma_max=80.0, rho=7.0, device=device)\n",
    "eval_model.eval() # Already done, but good practice\n",
    "\n",
    "with torch.no_grad():\n",
    "    while num_generated < NUM_IMAGES_FOR_FID:\n",
    "        current_batch_size = min(FID_BATCH_SIZE, NUM_IMAGES_FOR_FID - num_generated)\n",
    "        if current_batch_size == 0:\n",
    "            break\n",
    "        \n",
    "        images = sample_pc_heun(\n",
    "            model=eval_model,\n",
    "            shape=(current_batch_size, 3, 32, 32),\n",
    "            sigmas=sigmas,\n",
    "            device=device,\n",
    "            disable_tqdm=True\n",
    "        )\n",
    "        \n",
    "        images = (images.clamp(-1, 1) + 1) / 2\n",
    "        images = (images * 255).to(torch.uint8)\n",
    "        \n",
    "        for i in range(current_batch_size):\n",
    "            img_tensor = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            img = Image.fromarray(img_tensor)\n",
    "            img.save(os.path.join(GEN_DIR, f\"img_{num_generated + i}.png\"))\n",
    "        \n",
    "        num_generated += current_batch_size\n",
    "        print(f\"Generated {num_generated}/{NUM_IMAGES_FOR_FID} images...\")\n",
    "\n",
    "print(\"...Image generation complete.\")\n",
    "\n",
    "# --- Calculate FID ---\n",
    "print(\"Calculating FID score...\")\n",
    "fid_score = compute_fid(\n",
    "    GEN_DIR,\n",
    "    dataset_name=\"cifar10\",\n",
    "    dataset_res=32,\n",
    "    dataset_split=\"train\",\n",
    "    stats_file=stats_path,\n",
    "    mode=\"clean\"\n",
    ")\n",
    "\n",
    "# --- Health Check ---\n",
    "print(\"\\n--- Health Check ---\")\n",
    "print(\"====================================\")\n",
    "print(f\"   üöÄ FINAL FID SCORE: {fid_score:.4f} üöÄ\")\n",
    "print(\"====================================\")\n",
    "print(\"(The original paper's score is ~2.4 after full training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
