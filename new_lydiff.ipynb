{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "240d3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torch_fidelity lpips --quiet\n",
    "import math, random, os, functools, itertools, time\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional: LPIPS for RF pre-metric (installed above)\n",
    "try:\n",
    "    import lpips\n",
    "    LPIPS_AVAILABLE = True\n",
    "except Exception:\n",
    "    LPIPS_AVAILABLE = False\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d49fa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    dataset: str = 'CIFAR10'  # 'CIFAR10' or your dataset name\n",
    "    data_root: str = './data'\n",
    "    image_size: int = 32\n",
    "    channels: int = 3\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 128  # per-device batch size\n",
    "    scale_batch_by_gpus: bool = True\n",
    "    auto_scale_lr: bool = True\n",
    "    reference_batch_size: int = 128\n",
    "    reference_num_gpus: int = 1\n",
    "    num_steps: int = 300_000\n",
    "    lr: float = 2e-4\n",
    "    ema_decay: float = 0.999  # EMA for sampling stability\n",
    "    grad_clip: float = 1.0\n",
    "    use_scheduler: bool = True\n",
    "    scheduler_min_lr: float = 2e-5\n",
    "    mixed_precision: bool = True\n",
    "\n",
    "    # Objective toggle\n",
    "    objective: str = 'edm'  # 'edm' or 'flow' ('flow' = flow/rectified-flow matching)\n",
    "\n",
    "    # EDM noise schedule (Table 1, EDM)\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80.0\n",
    "    sigma_data: float = 0.5\n",
    "    rho: float = 7.0\n",
    "    P_mean: float = -1.2  # log-sigma mean\n",
    "    P_std: float = 1.2    # log-sigma std\n",
    "\n",
    "    # Sampler\n",
    "    NFE: int = 35  # number of function evaluations (Heun steps)\n",
    "    sample_bs: int = 64\n",
    "    save_dir: str = './samples'\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94882c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using effective batch size 512 (128 per GPU × 4 GPUs)\n"
     ]
    }
   ],
   "source": [
    "def make_dataloaders(cfg: Config):\n",
    "    device_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    batch_size = cfg.batch_size * device_count if cfg.scale_batch_by_gpus else cfg.batch_size\n",
    "    if device_count > 1 and cfg.scale_batch_by_gpus:\n",
    "        print(f'Using effective batch size {batch_size} ({cfg.batch_size} per GPU × {device_count} GPUs)')\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(cfg.image_size),\n",
    "        transforms.CenterCrop(cfg.image_size),\n",
    "        transforms.ToTensor(),  # [0,1]\n",
    "        # map to [-1,1] for EDM / standard diffusion\n",
    "        transforms.Lambda(lambda x: x * 2.0 - 1.0),\n",
    "    ])\n",
    "    if cfg.dataset.upper() == 'CIFAR10':\n",
    "        trainset = datasets.CIFAR10(root=cfg.data_root, train=True, download=True, transform=tfm)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Add your dataset here.\")\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
    "                             drop_last=True, num_workers=8, pin_memory=True)\n",
    "    return trainloader\n",
    "\n",
    "trainloader = make_dataloaders(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6ee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        t is (B,) of log-sigma (EDM) or time t in [0,1] (Flow). We still feed a scalar with sin-cos.\n",
    "        \"\"\"\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(torch.arange(half, device=t.device) * (-math.log(10_000) / max(half - 1, 1)))\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        if self.dim % 2 == 1:  # pad if odd\n",
    "            emb = F.pad(emb, (0,1))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c947c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, groups=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(groups, in_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(tdim, out_ch)\n",
    "        )\n",
    "        self.norm2 = nn.GroupNorm(groups, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.skip = (nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity())\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.conv1(self.act(self.norm1(x)))\n",
    "        h = h + self.emb(temb)[:, :, None, None]\n",
    "        h = self.conv2(self.act(self.norm2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, ch, heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, ch)\n",
    "        self.qkv = nn.Conv2d(ch, ch*3, 1)\n",
    "        self.proj = nn.Conv2d(ch, ch, 1)\n",
    "        self.heads = heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q,k,v = qkv.chunk(3, dim=1)\n",
    "        q = q.view(b, self.heads, c//self.heads, h*w)\n",
    "        k = k.view(b, self.heads, c//self.heads, h*w)\n",
    "        v = v.view(b, self.heads, c//self.heads, h*w)\n",
    "        attn = torch.softmax((q.transpose(-2,-1) @ k) / math.sqrt(c//self.heads), dim=-1)  # (B,H,HW,HW)\n",
    "        out = (attn @ v.transpose(-2,-1)).transpose(-2,-1).contiguous()\n",
    "        out = out.view(b, c, h, w)\n",
    "        return x + self.proj(out)\n",
    "\n",
    "class EDMUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet backbone. Outputs raw network F_theta; EDM preconditioning is applied outside.\n",
    "    Deep supervision: intermediate x0 heads to realize 'predictive coding' residual learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Config, ch=128, ch_mult=(1,2,2,2), num_res=2, attn_res=(16,)):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.attn_resolutions = set(attn_res)\n",
    "        self.in_conv = nn.Conv2d(cfg.channels, ch, 3, padding=1)\n",
    "        tdim = ch * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(tdim),\n",
    "            nn.Linear(tdim, tdim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(tdim, tdim),\n",
    "        )\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "\n",
    "        chs = []\n",
    "        curr_ch = ch\n",
    "        in_size = cfg.image_size\n",
    "        for level, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res):\n",
    "                block = ResBlock(curr_ch, out_ch, tdim)\n",
    "                self.downs.append(block)\n",
    "                curr_ch = out_ch\n",
    "                chs.append(curr_ch)  # track skip connection channels\n",
    "                if in_size in self.attn_resolutions:\n",
    "                    self.downs.append(AttentionBlock(curr_ch))\n",
    "            if level != len(ch_mult) - 1:\n",
    "                self.downs.append(nn.Conv2d(curr_ch, curr_ch, 3, stride=2, padding=1))\n",
    "                in_size //= 2\n",
    "\n",
    "        self.mid_block1 = ResBlock(curr_ch, curr_ch, tdim)\n",
    "        self.mid_attn = AttentionBlock(curr_ch)\n",
    "        self.mid_block2 = ResBlock(curr_ch, curr_ch, tdim)\n",
    "\n",
    "        aux_channels = [ch * mult for level, mult in reversed(list(enumerate(ch_mult))) if level != 0]\n",
    "        self.aux_heads = nn.ModuleList([\n",
    "            nn.Conv2d(c, cfg.channels, 1) for c in aux_channels\n",
    "        ])\n",
    "\n",
    "        for level, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res):\n",
    "                skip_ch = chs.pop()\n",
    "                self.ups.append(ResBlock(curr_ch + skip_ch, out_ch, tdim))\n",
    "                curr_ch = out_ch\n",
    "                if in_size in self.attn_resolutions:\n",
    "                    self.ups.append(AttentionBlock(curr_ch))\n",
    "            if level != 0:\n",
    "                self.ups.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "                in_size *= 2\n",
    "\n",
    "        self.out_norm = nn.GroupNorm(8, curr_ch)\n",
    "        self.out = nn.Conv2d(curr_ch, cfg.channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t_scalar):\n",
    "        temb = self.time_mlp(t_scalar)\n",
    "        hs = []\n",
    "        h = self.in_conv(x)\n",
    "        aux_preds = []\n",
    "        for m in self.downs:\n",
    "            if isinstance(m, ResBlock):\n",
    "                h = m(h, temb)\n",
    "                hs.append(h)\n",
    "            elif isinstance(m, AttentionBlock):\n",
    "                h = m(h)\n",
    "            else:\n",
    "                h = m(h)\n",
    "        h = self.mid_block1(h, temb)\n",
    "        h = self.mid_attn(h)\n",
    "        h = self.mid_block2(h, temb)\n",
    "        up_idx = 0\n",
    "        for m in self.ups:\n",
    "            if isinstance(m, ResBlock):\n",
    "                skip = hs.pop()\n",
    "                h = torch.cat([h, skip], dim=1)\n",
    "                h = m(h, temb)\n",
    "            elif isinstance(m, AttentionBlock):\n",
    "                h = m(h)\n",
    "            else:\n",
    "                h = m(h)\n",
    "                if up_idx < len(self.aux_heads):\n",
    "                    aux_preds.append(self.aux_heads[up_idx](h))\n",
    "                    up_idx += 1\n",
    "        h = self.out(self.out_norm(h))\n",
    "        return h, aux_preds  # raw network output F_theta (no preconditioning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0177a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_log_normal_sigma(bs, P_mean, P_std, device):\n",
    "    # Sample log sigma ~ N(P_mean, P_std^2)\n",
    "    return torch.exp(P_mean + P_std * torch.randn(bs, device=device))\n",
    "\n",
    "def karras_sigma_schedule(N, sigma_min, sigma_max, rho=7.0, device='cpu'):\n",
    "    # Monotone decreasing σ sequence used for sampling; see EDM Table 1 & Alg 1\n",
    "    ramp = torch.linspace(0, 1, N, device=device)\n",
    "    min_inv_rho = sigma_min**(1/rho)\n",
    "    max_inv_rho = sigma_max**(1/rho)\n",
    "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho))**rho\n",
    "    return sigmas\n",
    "\n",
    "class EDMPrecondWrapper(nn.Module):\n",
    "    def __init__(self, net: EDMUNet, sigma_data: float):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.sigma_data = sigma_data\n",
    "\n",
    "    def forward(self, x, sigma):\n",
    "        \"\"\"\n",
    "        x: (B,C,H,W) in [-1,1]\n",
    "        sigma: (B,) positive noise level\n",
    "        returns: denoised x0 estimate (EDM preconditioned output)\n",
    "        \"\"\"\n",
    "        sigma = sigma.view(-1, 1, 1, 1)\n",
    "        c_skip = (self.sigma_data**2) / (sigma**2 + self.sigma_data**2)\n",
    "        c_out  = (sigma * self.sigma_data) / torch.sqrt(sigma**2 + self.sigma_data**2)\n",
    "        c_in   = 1.0 / torch.sqrt(sigma**2 + self.sigma_data**2)\n",
    "        # c_noise is log-sigma / 4 per EDM\n",
    "        c_noise = 0.25 * torch.log(sigma.squeeze(-1).squeeze(-1).squeeze(-1))\n",
    "\n",
    "        f_raw, aux_preds = self.net(c_in * x, c_noise)\n",
    "        D = c_skip * x + c_out * f_raw\n",
    "        return D, aux_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469e03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_xt_rf(x0, eps, t):\n",
    "    # xt = (1-t) x0 + t eps ; eps ~ N(0,I), t in [0,1]\n",
    "    return (1.0 - t)[:, None, None, None] * x0 + t[:, None, None, None] * eps\n",
    "\n",
    "def flow_matching_targets(x0, eps):\n",
    "    # v = d/dt xt = eps - x0  (constant in t for linear path)\n",
    "    return eps - x0\n",
    "\n",
    "def u_shaped_t(bs, device):\n",
    "    # U-shaped distribution (more mass near 0 and 1), improves few-step performance\n",
    "    u = torch.rand(bs, device=device)\n",
    "    t = 0.5 - 0.5 * torch.cos(math.pi * u)\n",
    "    return t.clamp(1e-5, 1-1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6b56299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective batch size: 512. Scaled LR -> 8.00e-04\n"
     ]
    }
   ],
   "source": [
    "def make_model(cfg: Config):\n",
    "    net = EDMUNet(cfg)\n",
    "    model = EDMPrecondWrapper(net, sigma_data=cfg.sigma_data).to(DEVICE)\n",
    "    ema = EDMPrecondWrapper(EDMUNet(cfg), sigma_data=cfg.sigma_data).to(DEVICE)\n",
    "    ema.load_state_dict(model.state_dict())\n",
    "    for p in ema.parameters():\n",
    "        p.requires_grad = False\n",
    "    device_count = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "    effective_batch = cfg.batch_size * device_count if cfg.scale_batch_by_gpus else cfg.batch_size\n",
    "    cfg.effective_batch_size = effective_batch\n",
    "    reference_batch = max(1, cfg.reference_batch_size * max(1, cfg.reference_num_gpus))\n",
    "    scaled_lr = cfg.lr\n",
    "    if cfg.auto_scale_lr:\n",
    "        scaled_lr = cfg.lr * (effective_batch / reference_batch)\n",
    "    cfg.effective_lr = scaled_lr\n",
    "    if cfg.auto_scale_lr:\n",
    "        print(f\"Effective batch size: {effective_batch}. Scaled LR -> {scaled_lr:.2e}\")\n",
    "    opt = AdamW(model.parameters(), lr=scaled_lr, weight_decay=0.0, betas=(0.9, 0.999))\n",
    "    return model, ema, opt\n",
    "\n",
    "model, ema, opt = make_model(cfg)\n",
    "global_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d9c9d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling DataParallel across 4 GPUs.\n",
      "Using mixed precision (bf16); GradScaler enabled=False\n"
     ]
    }
   ],
   "source": [
    "def setup_data_parallel(model, ema):\n",
    "    if DEVICE != 'cuda':\n",
    "        print('CUDA not available; keeping single-device model.')\n",
    "        return model, ema\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    if num_devices <= 1:\n",
    "        print('Only one CUDA device detected; DataParallel not applied.')\n",
    "        return model, ema\n",
    "    print(f'Enabling DataParallel across {num_devices} GPUs.')\n",
    "    model = nn.DataParallel(model)\n",
    "    ema = ema.to(DEVICE)\n",
    "    return model, ema\n",
    "\n",
    "model, ema = setup_data_parallel(model, ema)\n",
    "\n",
    "def _normalize_state_dict_keys(state_dict):\n",
    "    if not any(key.startswith('module.') for key in state_dict.keys()):\n",
    "        return state_dict\n",
    "    return {key.replace('module.', '', 1): value for key, value in state_dict.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(ema, model, decay):\n",
    "    source_state = _normalize_state_dict_keys(model.state_dict())\n",
    "    target_state = ema.state_dict()\n",
    "    for key, param in target_state.items():\n",
    "        if param.dtype.is_floating_point:\n",
    "            param.copy_(decay * param + (1.0 - decay) * source_state[key])\n",
    "\n",
    "USE_BF16 = DEVICE == 'cuda' and torch.cuda.is_bf16_supported()\n",
    "AMP_DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "USE_MIXED_PRECISION = cfg.mixed_precision and DEVICE == 'cuda'\n",
    "scaler = GradScaler(enabled=USE_MIXED_PRECISION and not USE_BF16) if USE_MIXED_PRECISION else None\n",
    "if USE_MIXED_PRECISION:\n",
    "    prec = 'bf16' if USE_BF16 else 'fp16'\n",
    "    print(f'Using mixed precision ({prec}); GradScaler enabled={scaler is not None and scaler.is_enabled()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7788e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /blue/wdixon/wang.yixuan/.conda/envs/ly/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "Loading model from: /blue/wdixon/wang.yixuan/.conda/envs/ly/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "lpips_fn = lpips.LPIPS(net='vgg').to(DEVICE).eval() if LPIPS_AVAILABLE else None\n",
    "\n",
    "def loss_lpips_huber(x_hat, x0, delta=0.01):\n",
    "    # LPIPS + Huber (optional, RF low-NFE improvements)\n",
    "    loss = 0.0\n",
    "    if LPIPS_AVAILABLE:\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "        p = lpips_fn(torch.clamp((x_hat+1)/2,0,1), torch.clamp((x0+1)/2,0,1))\n",
    "        loss = p.mean()\n",
    "    # small Huber in pixel space\n",
    "    diff = x_hat - x0\n",
    "    huber = torch.where(diff.abs() < delta, 0.5*diff**2, delta*(diff.abs()-0.5*delta)).mean()\n",
    "    return loss + huber\n",
    "\n",
    "def train_step(batch, cfg: Config, model, ema, opt, objective='edm', scaler: Optional[GradScaler] = None):\n",
    "    global global_step\n",
    "    x0 = batch[0].to(DEVICE, non_blocking=True)  # in [-1,1]\n",
    "    B = x0.shape[0]\n",
    "    use_amp = USE_MIXED_PRECISION\n",
    "    if use_amp:\n",
    "        amp_context = autocast\n",
    "        amp_kwargs = {'device_type': 'cuda', 'dtype': AMP_DTYPE}\n",
    "        if scaler is None and AMP_DTYPE == torch.float16:\n",
    "            scaler = GradScaler(enabled=True)\n",
    "    else:\n",
    "        amp_context = nullcontext\n",
    "        amp_kwargs = {}\n",
    "\n",
    "    def edm_forward():\n",
    "        sigma = rand_log_normal_sigma(B, cfg.P_mean, cfg.P_std, DEVICE).clamp(cfg.sigma_min, cfg.sigma_max)\n",
    "        n = torch.randn_like(x0)\n",
    "        x_noisy = x0 + sigma[:, None, None, None] * n\n",
    "        xhat, aux_preds = model(x_noisy, sigma)\n",
    "        lam = (sigma**2 + cfg.sigma_data**2) / ((sigma * cfg.sigma_data)**2)\n",
    "        pix_loss = (lam[:, None, None, None] * (xhat - x0)**2).mean()\n",
    "        aux_loss = 0.0\n",
    "        for aux in aux_preds:\n",
    "            aux_loss += F.mse_loss(aux, F.interpolate(x0, aux.shape[-2:], mode='bilinear', align_corners=False))\n",
    "        aux_loss = 0.25 * aux_loss / max(1, len(aux_preds))\n",
    "        return pix_loss + aux_loss\n",
    "\n",
    "    def flow_forward():\n",
    "        t = u_shaped_t(B, DEVICE)\n",
    "        eps = torch.randn_like(x0)\n",
    "        xt = sample_xt_rf(x0, eps, t)\n",
    "        base_net = getattr(model, 'module', model).net\n",
    "        f_raw, aux_preds = base_net(xt, t)\n",
    "        v_target = flow_matching_targets(x0, eps)\n",
    "        loss_main = F.mse_loss(f_raw, v_target)\n",
    "        if LPIPS_AVAILABLE:\n",
    "            xhat = xt + f_raw * (1.0 - t)[:, None, None, None]\n",
    "            loss_main = loss_main + 0.05 * loss_lpips_huber(xhat, x0)\n",
    "        aux_loss = 0.0\n",
    "        for aux in aux_preds:\n",
    "            v_down = F.interpolate(v_target, aux.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            aux_loss += F.mse_loss(aux, v_down)\n",
    "        aux_loss = 0.25 * aux_loss / max(1, len(aux_preds))\n",
    "        return loss_main + aux_loss\n",
    "\n",
    "    with amp_context(**amp_kwargs):\n",
    "        if objective == 'edm':\n",
    "            loss = edm_forward()\n",
    "        elif objective == 'flow':\n",
    "            loss = flow_forward()\n",
    "        else:\n",
    "            raise ValueError(\"objective must be 'edm' or 'flow'\")\n",
    "\n",
    "    loss_value = float(loss.detach().cpu().item())\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    if scaler is not None and scaler.is_enabled():\n",
    "        scaler.scale(loss).backward()\n",
    "        if cfg.grad_clip is not None:\n",
    "            scaler.unscale_(opt)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        if cfg.grad_clip is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "        opt.step()\n",
    "    update_ema(ema, model, cfg.ema_decay)\n",
    "    global_step += 1\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba196ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def heun_sampler_ema(ema, cfg: Config, num: int = 64):\n",
    "    ema.eval()\n",
    "    sigmas = karras_sigma_schedule(cfg.NFE, cfg.sigma_min, cfg.sigma_max, cfg.rho, device=DEVICE)\n",
    "    x = torch.randn(num, cfg.channels, cfg.image_size, cfg.image_size, device=DEVICE) * sigmas[0]\n",
    "    for i in range(len(sigmas)-1):\n",
    "        sigma_i = sigmas[i]\n",
    "        sigma_j = sigmas[i+1]\n",
    "        # Derivative of x w.r.t. t (prob flow ODE form): dx/dt = -(σ̇/σ)[ x - D(x;σ) ]\n",
    "        # In discrete form with σ as the \"time\" parameterizing, EDM uses Heun (improved Euler):\n",
    "        #   d_i = ( (sigma_dot/sigma) * x - (sigma_dot) * D(x; sigma)/sigma )  -> implemented via EDM Alg 1\n",
    "        # We use the simpler practical Euler predictor + trapezoid corrector based on D(x;σ).\n",
    "        # (Exact EDM code uses the same structure; see Alg. 1 lines 4–8; guard at σ->0) :contentReference[oaicite:11]{index=11}\n",
    "\n",
    "        # Compute derivative at i\n",
    "        D_i, _ = ema(x, torch.full((num,), sigma_i, device=DEVICE))\n",
    "        d_i = (D_i - x) / (sigma_i**2) * (- (sigma_j - sigma_i)) * (sigma_i)  # scaled step; equivalent form\n",
    "        # Euler step\n",
    "        x_euler = x + d_i\n",
    "\n",
    "        if sigma_j > 0:\n",
    "            # 2nd-order correction (Heun/trapezoid)\n",
    "            D_j, _ = ema(x_euler, torch.full((num,), sigma_j, device=DEVICE))\n",
    "            d_j = (D_j - x_euler) / (sigma_j**2) * (- (sigma_j - sigma_i)) * (sigma_j)\n",
    "            x = x + 0.5 * (d_i + d_j)\n",
    "        else:\n",
    "            # At σ=0, use Euler (avoid division by zero)\n",
    "            x = x_euler\n",
    "\n",
    "    x = x.clamp(-1, 1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f477d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39996f48d3b549c38cd6ab0827fd4545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/300000 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(cfg: Config, model, ema, opt, trainloader, scaler: Optional[GradScaler] = None):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    data_iter = itertools.cycle(trainloader)\n",
    "    scheduler = None\n",
    "    if cfg.use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            opt, T_max=cfg.num_steps, eta_min=cfg.scheduler_min_lr\n",
    "        )\n",
    "    loss_ema = None\n",
    "    with tqdm(range(cfg.num_steps), desc=\"Training\", unit=\"step\") as progress:\n",
    "        for step in progress:\n",
    "            batch = next(data_iter)\n",
    "            loss = train_step(batch, cfg, model, ema, opt, objective=cfg.objective, scaler=scaler)\n",
    "            losses.append(loss)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            loss_ema = loss if loss_ema is None else 0.9 * loss_ema + 0.1 * loss\n",
    "            postfix = {\"loss\": f\"{loss_ema:.4f}\"}\n",
    "            if scheduler is not None:\n",
    "                postfix[\"lr\"] = f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "            progress.set_postfix(postfix)\n",
    "            if (step + 1) % 5000 == 0:\n",
    "                with torch.no_grad():\n",
    "                    samples = heun_sampler_ema(ema, cfg, num=cfg.sample_bs)\n",
    "                grid = vutils.make_grid((samples+1)/2, nrow=int(math.sqrt(cfg.sample_bs)))\n",
    "                vutils.save_image(grid, os.path.join(cfg.save_dir, f\"sample_{cfg.objective}_{step+1}.png\"))\n",
    "    return losses\n",
    "\n",
    "train(cfg, model, ema, opt, trainloader, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f7a9a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.2449,  ..., 0.3200, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 1.0000,  ..., 0.1597, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.3612,  ..., 0.1081, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 1.0000,  ..., 0.1802, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.3442,  ..., 0.0201, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 1.0000,  ..., 0.2813, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training (or loading a checkpoint), sample:\n",
    "with torch.no_grad():\n",
    "    samples = heun_sampler_ema(ema, cfg, num=cfg.sample_bs)\n",
    "grid = vutils.make_grid((samples+1)/2, nrow=int(math.sqrt(cfg.sample_bs)))\n",
    "vutils.save_image(grid, os.path.join(cfg.save_dir, f\"final_{cfg.objective}.png\"))\n",
    "grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661755a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
