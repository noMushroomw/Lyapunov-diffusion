{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1f265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, time, random, functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as tvu\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_all(42)\n",
    "\n",
    "def count_params(m: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecab0b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainCfg(dataset='CIFAR10', data_root='./data', channels=3, H=32, W=32, emb_dim=128, C_hidden=192, K=10, sigma_data=0.5, sigma_min=0.002, sigma_max=80.0, rho=7.0, batch_size=256, num_workers=4, epochs=50, lr=0.0002, wd=0.0, ema_decay=0.999, log_every=100)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TrainCfg:\n",
    "    # data\n",
    "    dataset: str = 'CIFAR10'\n",
    "    data_root: str = './data'\n",
    "    channels: int = 3\n",
    "    H: int = 32\n",
    "    W: int = 32\n",
    "    # model\n",
    "    emb_dim: int = 128\n",
    "    C_hidden: int = 192        # divisible by 32 â†’ safe for GroupNorm\n",
    "    K: int = 10                # predictive-coding iterations\n",
    "    # edm preconditioning\n",
    "    sigma_data: float = 0.5\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80.0\n",
    "    rho: float = 7.0           # EDM schedule exponent\n",
    "    # train\n",
    "    batch_size: int = 256\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 50\n",
    "    lr: float = 2e-4\n",
    "    wd: float = 0.0\n",
    "    ema_decay: float = 0.999\n",
    "    log_every: int = 100\n",
    "\n",
    "cfg = TrainCfg()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42185094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        half = dim // 2\n",
    "        # fixed frequencies\n",
    "        self.register_buffer('freqs', torch.exp(torch.linspace(math.log(1.0), math.log(10000.0), half)))\n",
    "\n",
    "    def forward(self, sigma: torch.Tensor):  # sigma: (B,) or (B,1,1,1)\n",
    "        sigma = sigma.view(-1)  # (B,)\n",
    "        x = sigma[:, None] * self.freqs[None, :]\n",
    "        emb = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "        if emb.shape[1] < self.dim:\n",
    "            emb = F.pad(emb, (0, self.dim - emb.shape[1]))\n",
    "        return emb  # (B, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a870f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _groups_for_channels(C: int, max_groups: int = 32) -> int:\n",
    "    # largest divisor of C not exceeding max_groups\n",
    "    for g in reversed(range(1, max_groups + 1)):\n",
    "        if C % g == 0:\n",
    "            return g\n",
    "    return 1\n",
    "\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, emb_dim: int, hidden: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, 2 * hidden)\n",
    "        )\n",
    "\n",
    "    def forward(self, emb: torch.Tensor):\n",
    "        # returns gamma, beta of shape (B, hidden)\n",
    "        gb = self.net(emb)\n",
    "        g, b = gb.chunk(2, dim=1)\n",
    "        return g, b\n",
    "\n",
    "class PCBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One predictive-coding update:\n",
    "      e = x_sigma - x_hat\n",
    "      h = Conv(cat[x_sigma, e]) -> GN -> SiLU -> FiLM -> Conv\n",
    "      delta = Conv(h) -> add to x_hat\n",
    "    All GroupNorm on hidden channels only.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, hidden: int, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_ch * 2, hidden, 3, padding=1)\n",
    "        g = _groups_for_channels(hidden, 32)\n",
    "        self.norm_h = nn.GroupNorm(g, hidden)\n",
    "        self.film = FiLM(emb_dim, hidden)\n",
    "        self.act = nn.SiLU()\n",
    "        self.conv_mid = nn.Conv2d(hidden, hidden, 3, padding=1)\n",
    "        self.norm_mid = nn.GroupNorm(g, hidden)\n",
    "        self.conv_out = nn.Conv2d(hidden, in_ch, 3, padding=1)\n",
    "\n",
    "    def forward(self, x_sigma: torch.Tensor, x_hat: torch.Tensor, emb: torch.Tensor):\n",
    "        e = x_sigma - x_hat\n",
    "        h = self.conv_in(torch.cat([x_sigma, e], dim=1))\n",
    "        h = self.norm_h(h)\n",
    "        g, b = self.film(emb)  # (B, hidden)\n",
    "        g = g[:, :, None, None]\n",
    "        b = b[:, :, None, None]\n",
    "        h = h * (1 + g) + b\n",
    "        h = self.act(h)\n",
    "        h = self.conv_mid(h)\n",
    "        h = self.norm_mid(h)\n",
    "        h = self.act(h)\n",
    "        delta = self.conv_out(h)\n",
    "        return delta\n",
    "\n",
    "class PredictiveCodingCore(nn.Module):\n",
    "    def __init__(self, in_ch: int, hidden: int, K: int, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([PCBlock(in_ch, hidden, emb_dim) for _ in range(K)])\n",
    "        self.final = nn.Conv2d(in_ch, in_ch, 1)\n",
    "\n",
    "    def forward(self, x_sigma: torch.Tensor, emb: torch.Tensor):\n",
    "        # start from zero estimate; update K times\n",
    "        x_hat = torch.zeros_like(x_sigma)\n",
    "        for blk in self.blocks:\n",
    "            x_hat = x_hat + blk(x_sigma, x_hat, emb)\n",
    "        return self.final(x_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d046c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiserEDM(nn.Module):\n",
    "    \"\"\"\n",
    "    Preconditioned network (Karras et al. 2022 EDM style):\n",
    "      y = cskip * x + cout * F(cin * x, emb)\n",
    "    The core F returns a residual in image space.\n",
    "    \"\"\"\n",
    "    def __init__(self, core: nn.Module, emb_dim: int, sigma_data: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.sigma_data = sigma_data\n",
    "        self.emb = FourierEmbedding(emb_dim)\n",
    "\n",
    "    def _coeffs(self, sigma: torch.Tensor):\n",
    "        # sigma: (B,1,1,1)\n",
    "        s2 = sigma**2\n",
    "        sd2 = self.sigma_data**2\n",
    "        cskip = sd2 / (s2 + sd2)\n",
    "        cin   = 1.0 / torch.sqrt(s2 + sd2)\n",
    "        cout  = sigma * self.sigma_data / torch.sqrt(s2 + sd2)\n",
    "        return cskip, cout, cin\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sigma: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B,C,H,W)\n",
    "        sigma: (B,) or (B,1,1,1)\n",
    "        returns denoised prediction at noise level sigma\n",
    "        \"\"\"\n",
    "        if sigma.dim() == 1:\n",
    "            sigma_img = sigma[:, None, None, None]\n",
    "        else:\n",
    "            sigma_img = sigma\n",
    "            sigma = sigma.view(-1)\n",
    "\n",
    "        cskip, cout, cin = self._coeffs(sigma_img)\n",
    "        x_in = cin * x\n",
    "        emb = self.emb(torch.log(sigma + 1e-8))  # use log sigma\n",
    "        h = self.core(x_in, emb)\n",
    "        y = cskip * x + cout * h\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0dc332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def edm_sigma_schedule(steps: int, sigma_min: float, sigma_max: float, rho: float, device):\n",
    "    i = torch.arange(steps, device=device, dtype=torch.float32)\n",
    "    t = sigma_max**(1/rho) + (i / (steps - 1)) * (sigma_min**(1/rho) - sigma_max**(1/rho))\n",
    "    sigmas = t**rho\n",
    "    return sigmas  # decreasing\n",
    "\n",
    "@torch.no_grad()\n",
    "def heun_sampler(denoiser: nn.Module, batch_size: int, channels: int, height: int, width: int,\n",
    "                 steps: int = 40, sigma_min: float = 0.002, sigma_max: float = 80.0, rho: float = 7.0,\n",
    "                 device=device):\n",
    "    sigmas = edm_sigma_schedule(steps, sigma_min, sigma_max, rho, device)\n",
    "    # start from N(0, sigma_max^2 I)\n",
    "    x = torch.randn(batch_size, channels, height, width, device=device) * sigmas[0]\n",
    "    for i in range(steps - 1):\n",
    "        s_i = sigmas[i]\n",
    "        s_j = sigmas[i + 1]\n",
    "        s_i_img = s_i.view(1, 1, 1, 1)\n",
    "        s_j_img = s_j.view(1, 1, 1, 1)\n",
    "        # velocity at s_i\n",
    "        d_i = (denoiser(x, s_i.expand(batch_size)) - x) / s_i_img\n",
    "        x_euler = x + (s_j - s_i) * d_i\n",
    "        # velocity at s_j\n",
    "        d_j = (denoiser(x_euler, s_j.expand(batch_size)) - x_euler) / s_j_img\n",
    "        # Heun (trapezoid)\n",
    "        x = x + (s_j - s_i) * 0.5 * (d_i + d_j)\n",
    "    # final clamp to [-1,1]\n",
    "    return x.clamp(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96edbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sigma(batch_size: int, sigma_min: float, sigma_max: float, device):\n",
    "    u = torch.rand(batch_size, device=device)\n",
    "    log_min, log_max = math.log(sigma_min), math.log(sigma_max)\n",
    "    sigma = torch.exp(u * (log_max - log_min) + log_min)\n",
    "    return sigma  # (B,)\n",
    "\n",
    "def edm_loss(denoiser: nn.Module, x0: torch.Tensor, sigma: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Train target is the clean image x0; denoiser(x_noisy, sigma) should predict clean x.\n",
    "    \"\"\"\n",
    "    if sigma.dim() == 1:\n",
    "        sigma_img = sigma[:, None, None, None]\n",
    "    else:\n",
    "        sigma_img = sigma\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_noisy = x0 + sigma_img * noise\n",
    "    x_pred = denoiser(x_noisy, sigma)\n",
    "    loss = F.mse_loss(x_pred, x0, reduction='mean')\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d4b74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.collected = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.data.clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad: \n",
    "                continue\n",
    "            assert name in self.shadow\n",
    "            new_avg = self.decay * self.shadow[name] + (1.0 - self.decay) * p.data\n",
    "            self.shadow[name] = new_avg.clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model: nn.Module):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad: \n",
    "                continue\n",
    "            p.data.copy_(self.shadow[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2bc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10(root: str, batch_size: int, num_workers: int):\n",
    "    tfm = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])  # to [-1,1]\n",
    "    ])\n",
    "    ds = tv.datasets.CIFAR10(root=root, train=True, download=True, transform=tfm)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "    return dl\n",
    "\n",
    "train_loader = get_cifar10(cfg.data_root, cfg.batch_size, cfg.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d70e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params (M): 3.980202\n"
     ]
    }
   ],
   "source": [
    "core = PredictiveCodingCore(in_ch=cfg.channels, hidden=cfg.C_hidden, K=cfg.K, emb_dim=cfg.emb_dim)\n",
    "denoiser = DenoiserEDM(core=core, emb_dim=cfg.emb_dim, sigma_data=cfg.sigma_data).to(device)\n",
    "print('Params (M):', count_params(denoiser)/1e6)\n",
    "\n",
    "opt = torch.optim.AdamW(denoiser.parameters(), lr=cfg.lr, weight_decay=cfg.wd, betas=(0.9, 0.999))\n",
    "ema = EMA(denoiser, decay=cfg.ema_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8d6dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 001 it 00099 | loss 0.09555\n",
      "epoch 001 | time 12.5s\n",
      "ep 002 it 00004 | loss 0.00382\n",
      "ep 002 it 00104 | loss 0.07575\n",
      "epoch 002 | time 11.9s\n",
      "ep 003 it 00009 | loss 0.00736\n",
      "ep 003 it 00109 | loss 0.07374\n",
      "epoch 003 | time 11.8s\n",
      "ep 004 it 00014 | loss 0.01064\n",
      "ep 004 it 00114 | loss 0.07252\n",
      "epoch 004 | time 11.8s\n",
      "ep 005 it 00019 | loss 0.01455\n",
      "ep 005 it 00119 | loss 0.07025\n",
      "epoch 005 | time 12.0s\n",
      "ep 006 it 00024 | loss 0.01792\n",
      "ep 006 it 00124 | loss 0.07039\n",
      "epoch 006 | time 12.1s\n",
      "ep 007 it 00029 | loss 0.02105\n",
      "ep 007 it 00129 | loss 0.07088\n",
      "epoch 007 | time 11.9s\n",
      "ep 008 it 00034 | loss 0.02415\n",
      "ep 008 it 00134 | loss 0.06960\n",
      "epoch 008 | time 11.9s\n",
      "ep 009 it 00039 | loss 0.02821\n",
      "ep 009 it 00139 | loss 0.07117\n",
      "epoch 009 | time 11.8s\n",
      "ep 010 it 00044 | loss 0.03098\n",
      "ep 010 it 00144 | loss 0.06960\n",
      "epoch 010 | time 11.8s\n",
      "ep 011 it 00049 | loss 0.03531\n",
      "ep 011 it 00149 | loss 0.06883\n",
      "epoch 011 | time 11.9s\n",
      "ep 012 it 00054 | loss 0.03765\n",
      "ep 012 it 00154 | loss 0.07001\n",
      "epoch 012 | time 11.8s\n",
      "ep 013 it 00059 | loss 0.04105\n",
      "ep 013 it 00159 | loss 0.06839\n",
      "epoch 013 | time 11.8s\n",
      "ep 014 it 00064 | loss 0.04384\n",
      "ep 014 it 00164 | loss 0.06893\n",
      "epoch 014 | time 11.9s\n",
      "ep 015 it 00069 | loss 0.04799\n",
      "ep 015 it 00169 | loss 0.06854\n",
      "epoch 015 | time 11.9s\n",
      "ep 016 it 00074 | loss 0.05109\n",
      "ep 016 it 00174 | loss 0.06796\n",
      "epoch 016 | time 11.9s\n",
      "ep 017 it 00079 | loss 0.05472\n",
      "ep 017 it 00179 | loss 0.06872\n",
      "epoch 017 | time 11.8s\n",
      "ep 018 it 00084 | loss 0.05785\n",
      "ep 018 it 00184 | loss 0.06848\n",
      "epoch 018 | time 11.8s\n",
      "ep 019 it 00089 | loss 0.06228\n",
      "ep 019 it 00189 | loss 0.06894\n",
      "epoch 019 | time 11.8s\n",
      "ep 020 it 00094 | loss 0.06536\n",
      "ep 020 it 00194 | loss 0.06849\n",
      "epoch 020 | time 11.9s\n",
      "ep 021 it 00099 | loss 0.06773\n",
      "epoch 021 | time 11.8s\n",
      "ep 022 it 00004 | loss 0.00331\n",
      "ep 022 it 00104 | loss 0.06837\n",
      "epoch 022 | time 11.8s\n",
      "ep 023 it 00009 | loss 0.00686\n",
      "ep 023 it 00109 | loss 0.06783\n",
      "epoch 023 | time 11.9s\n",
      "ep 024 it 00014 | loss 0.01015\n",
      "ep 024 it 00114 | loss 0.06897\n",
      "epoch 024 | time 11.8s\n",
      "ep 025 it 00019 | loss 0.01349\n",
      "ep 025 it 00119 | loss 0.06897\n",
      "epoch 025 | time 11.8s\n",
      "ep 026 it 00024 | loss 0.01736\n",
      "ep 026 it 00124 | loss 0.06678\n",
      "epoch 026 | time 11.8s\n",
      "ep 027 it 00029 | loss 0.01973\n",
      "ep 027 it 00129 | loss 0.06904\n",
      "epoch 027 | time 11.8s\n",
      "ep 028 it 00034 | loss 0.02374\n",
      "ep 028 it 00134 | loss 0.06715\n",
      "epoch 028 | time 12.0s\n",
      "ep 029 it 00039 | loss 0.02712\n",
      "ep 029 it 00139 | loss 0.06750\n",
      "epoch 029 | time 12.1s\n",
      "ep 030 it 00044 | loss 0.03114\n",
      "ep 030 it 00144 | loss 0.06740\n",
      "epoch 030 | time 11.8s\n",
      "ep 031 it 00049 | loss 0.03395\n",
      "ep 031 it 00149 | loss 0.06728\n",
      "epoch 031 | time 11.8s\n",
      "ep 032 it 00054 | loss 0.03700\n",
      "ep 032 it 00154 | loss 0.06793\n",
      "epoch 032 | time 11.8s\n",
      "ep 033 it 00059 | loss 0.04048\n",
      "ep 033 it 00159 | loss 0.06769\n",
      "epoch 033 | time 11.9s\n",
      "ep 034 it 00064 | loss 0.04371\n",
      "ep 034 it 00164 | loss 0.06808\n",
      "epoch 034 | time 11.8s\n",
      "ep 035 it 00069 | loss 0.04694\n",
      "ep 035 it 00169 | loss 0.06973\n",
      "epoch 035 | time 11.8s\n",
      "ep 036 it 00074 | loss 0.05150\n",
      "ep 036 it 00174 | loss 0.06711\n",
      "epoch 036 | time 11.8s\n",
      "ep 037 it 00079 | loss 0.05427\n",
      "ep 037 it 00179 | loss 0.06802\n",
      "epoch 037 | time 11.8s\n",
      "ep 038 it 00084 | loss 0.05698\n",
      "ep 038 it 00184 | loss 0.06683\n",
      "epoch 038 | time 11.8s\n",
      "ep 039 it 00089 | loss 0.06082\n",
      "ep 039 it 00189 | loss 0.06754\n",
      "epoch 039 | time 11.9s\n",
      "ep 040 it 00094 | loss 0.06429\n",
      "ep 040 it 00194 | loss 0.06747\n",
      "epoch 040 | time 11.9s\n",
      "ep 041 it 00099 | loss 0.06787\n",
      "epoch 041 | time 11.8s\n",
      "ep 042 it 00004 | loss 0.00315\n",
      "ep 042 it 00104 | loss 0.06711\n",
      "epoch 042 | time 11.8s\n",
      "ep 043 it 00009 | loss 0.00668\n",
      "ep 043 it 00109 | loss 0.06713\n",
      "epoch 043 | time 11.8s\n",
      "ep 044 it 00014 | loss 0.01043\n",
      "ep 044 it 00114 | loss 0.06734\n",
      "epoch 044 | time 11.8s\n",
      "ep 045 it 00019 | loss 0.01349\n",
      "ep 045 it 00119 | loss 0.06682\n",
      "epoch 045 | time 11.8s\n",
      "ep 046 it 00024 | loss 0.01667\n",
      "ep 046 it 00124 | loss 0.06722\n",
      "epoch 046 | time 11.8s\n",
      "ep 047 it 00029 | loss 0.01967\n",
      "ep 047 it 00129 | loss 0.06555\n",
      "epoch 047 | time 11.8s\n",
      "ep 048 it 00034 | loss 0.02393\n",
      "ep 048 it 00134 | loss 0.06586\n",
      "epoch 048 | time 11.8s\n",
      "ep 049 it 00039 | loss 0.02685\n",
      "ep 049 it 00139 | loss 0.06762\n",
      "epoch 049 | time 11.8s\n",
      "ep 050 it 00044 | loss 0.03010\n",
      "ep 050 it 00144 | loss 0.06753\n",
      "epoch 050 | time 11.9s\n"
     ]
    }
   ],
   "source": [
    "def train(model: nn.Module, opt, ema: EMA, loader, epochs: int, cfg: TrainCfg):\n",
    "    model.train()\n",
    "    global_step = 0\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        run_loss = 0.0\n",
    "        for it, (x, _) in enumerate(loader):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            sigma = sample_sigma(x.size(0), cfg.sigma_min, cfg.sigma_max, device)\n",
    "            loss = edm_loss(model, x, sigma)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            ema.update(model)\n",
    "\n",
    "            run_loss += loss.item()\n",
    "            global_step += 1\n",
    "            if global_step % cfg.log_every == 0:\n",
    "                print(f'ep {ep:03d} it {it:05d} | loss {run_loss/cfg.log_every:.5f}')\n",
    "                run_loss = 0.0\n",
    "        print(f'epoch {ep:03d} | time {time.time()-t0:.1f}s')\n",
    "\n",
    "# Example: train for a few epochs to sanity-check (increase to cfg.epochs for full run)\n",
    "train(denoiser, opt, ema, train_loader, epochs=50, cfg=cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f28e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to samples/edm_pc_cifar10.png\n"
     ]
    }
   ],
   "source": [
    "# Copy EMA weights into the model for sampling\n",
    "ema.copy_to(denoiser)\n",
    "denoiser.eval()\n",
    "with torch.no_grad():\n",
    "    imgs = heun_sampler(\n",
    "        denoiser,\n",
    "        batch_size=16,\n",
    "        channels=cfg.channels,\n",
    "        height=cfg.H,\n",
    "        width=cfg.W,\n",
    "        steps=40,\n",
    "        sigma_min=cfg.sigma_min,\n",
    "        sigma_max=cfg.sigma_max,\n",
    "        rho=cfg.rho,\n",
    "        device=device\n",
    "    )\n",
    "grid = tvu.make_grid((imgs + 1) * 0.5, nrow=4)  # back to [0,1]\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "tv.utils.save_image(grid, 'samples/edm_pc_cifar10.png')\n",
    "print('Saved to samples/edm_pc_cifar10.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b90ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
