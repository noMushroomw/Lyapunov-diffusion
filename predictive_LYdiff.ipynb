{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6b90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "‚úÖ Imports loaded\n"
     ]
    }
   ],
   "source": [
    "import math, os, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as tvu\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "def seed_all(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "seed_all(42)\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5219dd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(data_root='./data', channels=3, image_size=32, num_layers=4, resolutions=(32, 16, 8, 4), layer_channels=(64, 128, 256, 256), pc_iterations=8, emb_dim=128, sigma_data=0.5, sigma_min=0.002, sigma_max=80.0, rho=7.0, batch_size=128, num_workers=4, epochs=100, lr=0.0002, ema_decay=0.9999, log_every=100, sample_every=10, fid_samples=10000, fid_batch=100)\n",
      "\n",
      "‚úÖ Config created\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    data_root: str = './data'\n",
    "    channels: int = 3\n",
    "    image_size: int = 32\n",
    "    \n",
    "    # PC Architecture - Following hierarchical PC theory\n",
    "    num_layers: int = 4  # Number of hierarchical latent layers\n",
    "    resolutions: Tuple[int] = (32, 16, 8, 4)  # Resolution of each layer\n",
    "    layer_channels: Tuple[int] = (64, 128, 256, 256)  # Channels at each layer\n",
    "    pc_iterations: int = 8  # Inference iterations to minimize energy\n",
    "    \n",
    "    # Time embedding\n",
    "    emb_dim: int = 128\n",
    "    \n",
    "    # EDM preconditioning\n",
    "    sigma_data: float = 0.5\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80.0\n",
    "    rho: float = 7.0\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 128\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 100\n",
    "    lr: float = 2e-4\n",
    "    ema_decay: float = 0.9999\n",
    "    log_every: int = 100\n",
    "    sample_every: int = 10  # Sample every N epochs\n",
    "    \n",
    "    # FID evaluation\n",
    "    fid_samples: int = 10000\n",
    "    fid_batch: int = 100\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)\n",
    "print(\"\\n‚úÖ Config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcf6d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SinusoidalEmbedding\n"
     ]
    }
   ],
   "source": [
    "class SinusoidalEmbedding(nn.Module):\n",
    "    \"\"\"Time/sigma embedding.\"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        half = dim // 2\n",
    "        self.register_buffer('freqs', torch.exp(torch.linspace(math.log(1.0), math.log(1000.0), half)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.view(-1)[:, None] * self.freqs[None, :]\n",
    "        emb = torch.cat([x.sin(), x.cos()], dim=-1)\n",
    "        if emb.shape[1] < self.dim:\n",
    "            emb = F.pad(emb, (0, self.dim - emb.shape[1]))\n",
    "        return emb\n",
    "\n",
    "# Test\n",
    "emb_test = SinusoidalEmbedding(128)\n",
    "assert emb_test(torch.randn(4)).shape == (4, 128)\n",
    "print(\"‚úÖ SinusoidalEmbedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d042948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PCLayer\n"
     ]
    }
   ],
   "source": [
    "def get_groups(channels: int, max_groups: int = 32) -> int:\n",
    "    \"\"\"Find largest divisor for GroupNorm.\"\"\"\n",
    "    for g in range(max_groups, 0, -1):\n",
    "        if channels % g == 0:\n",
    "            return g\n",
    "    return 1\n",
    "\n",
    "class PCLayer(nn.Module):\n",
    "    \"\"\"One layer in the PC hierarchy.\n",
    "    \n",
    "    Receives top-down prediction from layer above,\n",
    "    compares with current state, computes error.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, emb_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Top-down prediction pathway (from layer above)\n",
    "        self.pred_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(get_groups(out_channels), out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Error processing (bottom-up)\n",
    "        self.error_conv = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.GroupNorm(get_groups(out_channels), out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Time embedding injection\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, out_channels * 2)\n",
    "        )\n",
    "    \n",
    "    def predict(self, x_above: torch.Tensor, temb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Top-down prediction: what does layer above predict for this layer?\"\"\"\n",
    "        h = self.pred_conv(x_above)\n",
    "        \n",
    "        # Add time conditioning\n",
    "        scale, shift = self.temb_proj(temb).chunk(2, dim=1)\n",
    "        h = h * (1 + scale[:, :, None, None]) + shift[:, :, None, None]\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def process_error(self, error: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Process prediction error (bottom-up pathway).\"\"\"\n",
    "        return self.error_conv(error)\n",
    "\n",
    "# Test\n",
    "layer_test = PCLayer(64, 128, 128).to(device)\n",
    "x_test = torch.randn(2, 64, 16, 16).to(device)\n",
    "temb_test = torch.randn(2, 128).to(device)\n",
    "pred = layer_test.predict(x_test, temb_test)\n",
    "error = layer_test.process_error(torch.randn(2, 128, 16, 16).to(device))\n",
    "assert pred.shape == (2, 128, 16, 16)\n",
    "assert error.shape == (2, 128, 16, 16)\n",
    "print(\"‚úÖ PCLayer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f7cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HierarchicalPC: torch.Size([2, 3, 32, 32]) -> torch.Size([2, 3, 32, 32])\n",
      "   Parameters: 5.95M\n",
      "   Latent hierarchy: (32, 16, 8, 4)\n",
      "   PC iterations: 8\n"
     ]
    }
   ],
   "source": [
    "class HierarchicalPC(nn.Module):\n",
    "    \"\"\"Hierarchical Predictive Coding Network.\n",
    "    \n",
    "    Following classical PC theory with:\n",
    "    - Multiple latent layers at different resolutions\n",
    "    - Top-down predictions\n",
    "    - Bottom-up error propagation\n",
    "    - Iterative inference to minimize energy\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        layer_channels: Tuple[int] = (64, 128, 256, 256),\n",
    "        resolutions: Tuple[int] = (32, 16, 8, 4),\n",
    "        pc_iterations: int = 8,\n",
    "        emb_dim: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(layer_channels)\n",
    "        self.resolutions = resolutions\n",
    "        self.layer_channels = layer_channels\n",
    "        self.pc_iterations = pc_iterations\n",
    "        \n",
    "        # Input projection (noisy image ‚Üí first latent layer)\n",
    "        self.input_proj = nn.Conv2d(in_channels, layer_channels[0], 3, padding=1)\n",
    "        \n",
    "        # PC layers (each layer predicts the one below)\n",
    "        self.pc_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.pc_layers.append(\n",
    "                PCLayer(layer_channels[i+1], layer_channels[i], emb_dim)\n",
    "            )\n",
    "        \n",
    "        # Downsampling (for hierarchy)\n",
    "        self.downsample = nn.ModuleList([\n",
    "            nn.Conv2d(layer_channels[i], layer_channels[i+1], 3, stride=2, padding=1)\n",
    "            for i in range(self.num_layers - 1)\n",
    "        ])\n",
    "        \n",
    "        # Upsampling (for top-down predictions)\n",
    "        self.upsample = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(layer_channels[i+1], layer_channels[i+1], 4, stride=2, padding=1)\n",
    "            for i in range(self.num_layers - 1)\n",
    "        ])\n",
    "        \n",
    "        # Output projection (first latent layer ‚Üí clean image)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.GroupNorm(get_groups(layer_channels[0]), layer_channels[0]),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(layer_channels[0], in_channels, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_noisy: torch.Tensor, temb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predictive coding inference.\n",
    "        \n",
    "        1. Initialize latent hierarchy from input\n",
    "        2. Iterate to minimize prediction errors\n",
    "        3. Output denoised prediction\n",
    "        \"\"\"\n",
    "        B = x_noisy.shape[0]\n",
    "        \n",
    "        # Initialize latent hierarchy\n",
    "        x = [None] * self.num_layers\n",
    "        x[0] = self.input_proj(x_noisy)\n",
    "        \n",
    "        # Build initial hierarchy via downsampling\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x[i+1] = self.downsample[i](x[i])\n",
    "        \n",
    "        # Predictive coding iterations\n",
    "        for it in range(self.pc_iterations):\n",
    "            # Top-down predictions and error computation\n",
    "            predictions = [None] * (self.num_layers - 1)\n",
    "            errors = [None] * (self.num_layers - 1)\n",
    "            \n",
    "            for i in range(self.num_layers - 1):\n",
    "                # Layer i+1 predicts layer i\n",
    "                x_above_upsampled = self.upsample[i](x[i+1])\n",
    "                predictions[i] = self.pc_layers[i].predict(x_above_upsampled, temb)\n",
    "                \n",
    "                # Prediction error\n",
    "                errors[i] = x[i] - predictions[i]\n",
    "            \n",
    "            # Update latent variables based on errors\n",
    "            # (In full PC, this would be gradient-based, but we use residual updates)\n",
    "            for i in range(self.num_layers - 1):\n",
    "                # Process error\n",
    "                error_processed = self.pc_layers[i].process_error(errors[i])\n",
    "                \n",
    "                # Update lower layer (move toward prediction)\n",
    "                x[i] = x[i] * 0.5 + predictions[i] * 0.5 + error_processed * 0.1\n",
    "                \n",
    "                # Propagate error up (update higher layer)\n",
    "                if i < self.num_layers - 1:\n",
    "                    error_down = self.downsample[i](error_processed)\n",
    "                    x[i+1] = x[i+1] + error_down * 0.1\n",
    "        \n",
    "        # Output denoised image\n",
    "        return self.output_proj(x[0])\n",
    "\n",
    "# Test\n",
    "pc_net = HierarchicalPC(\n",
    "    in_channels=3,\n",
    "    layer_channels=cfg.layer_channels,\n",
    "    resolutions=cfg.resolutions,\n",
    "    pc_iterations=cfg.pc_iterations,\n",
    "    emb_dim=cfg.emb_dim\n",
    ").to(device)\n",
    "\n",
    "x_test = torch.randn(2, 3, 32, 32).to(device)\n",
    "temb_test = torch.randn(2, 128).to(device)\n",
    "with torch.no_grad():\n",
    "    out = pc_net(x_test, temb_test)\n",
    "assert out.shape == (2, 3, 32, 32)\n",
    "print(f\"‚úÖ HierarchicalPC: {x_test.shape} -> {out.shape}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in pc_net.parameters())/1e6:.2f}M\")\n",
    "print(f\"   Latent hierarchy: {cfg.resolutions}\")\n",
    "print(f\"   PC iterations: {cfg.pc_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8322c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DenoiserEDM: torch.Size([2, 3, 32, 32]) -> torch.Size([2, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class DenoiserEDM(nn.Module):\n",
    "    \"\"\"EDM-preconditioned denoiser with PC core.\"\"\"\n",
    "    def __init__(self, core: nn.Module, emb_dim: int, sigma_data: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.sigma_data = sigma_data\n",
    "        self.emb = SinusoidalEmbedding(emb_dim)\n",
    "\n",
    "    def _coeffs(self, sigma: torch.Tensor):\n",
    "        s2 = sigma**2\n",
    "        sd2 = self.sigma_data**2\n",
    "        c_skip = sd2 / (s2 + sd2)\n",
    "        c_in = 1.0 / torch.sqrt(s2 + sd2)\n",
    "        c_out = sigma * self.sigma_data / torch.sqrt(s2 + sd2)\n",
    "        return c_skip, c_in, c_out\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sigma: torch.Tensor):\n",
    "        if sigma.dim() == 1:\n",
    "            sigma_img = sigma[:, None, None, None]\n",
    "        else:\n",
    "            sigma_img = sigma\n",
    "            sigma = sigma.view(-1)\n",
    "\n",
    "        c_skip, c_in, c_out = self._coeffs(sigma_img)\n",
    "        x_in = c_in * x\n",
    "        emb = self.emb(torch.log(sigma.clamp(min=1e-8)))\n",
    "        h = self.core(x_in, emb)\n",
    "        return c_skip * x + c_out * h\n",
    "\n",
    "# Test\n",
    "denoiser_test = DenoiserEDM(pc_net, cfg.emb_dim, cfg.sigma_data).to(device)\n",
    "x_test = torch.randn(2, 3, 32, 32).to(device)\n",
    "sigma_test = torch.rand(2).to(device) * 80\n",
    "with torch.no_grad():\n",
    "    out = denoiser_test(x_test, sigma_test)\n",
    "assert out.shape == (2, 3, 32, 32)\n",
    "print(f\"‚úÖ DenoiserEDM: {x_test.shape} -> {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c10c60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sampler: 41 steps, œÉ ‚àà [80.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def edm_schedule(steps: int, sigma_min: float, sigma_max: float, rho: float, device):\n",
    "    \"\"\"EDM noise schedule.\"\"\"\n",
    "    i = torch.arange(steps, device=device, dtype=torch.float32)\n",
    "    t = sigma_max**(1/rho) + (i / (steps - 1)) * (sigma_min**(1/rho) - sigma_max**(1/rho))\n",
    "    return torch.cat([t**rho, torch.zeros(1, device=device)])\n",
    "\n",
    "@torch.no_grad()\n",
    "def heun_sampler(\n",
    "    denoiser: nn.Module,\n",
    "    batch_size: int,\n",
    "    channels: int,\n",
    "    size: int,\n",
    "    steps: int = 50,\n",
    "    sigma_min: float = 0.002,\n",
    "    sigma_max: float = 80.0,\n",
    "    rho: float = 7.0,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"Heun sampler with CORRECT denoising residual (no division!).\"\"\"\n",
    "    sigmas = edm_schedule(steps, sigma_min, sigma_max, rho, device)\n",
    "    x = torch.randn(batch_size, channels, size, size, device=device) * sigmas[0]\n",
    "    \n",
    "    for i in range(steps):\n",
    "        sigma_cur = sigmas[i]\n",
    "        sigma_next = sigmas[i + 1]\n",
    "        \n",
    "        if sigma_next == 0:\n",
    "            x = denoiser(x, sigma_cur.expand(batch_size))\n",
    "            break\n",
    "        \n",
    "        # ‚úÖ CORRECT: No division by sigma!\n",
    "        d_cur = denoiser(x, sigma_cur.expand(batch_size)) - x\n",
    "        x_euler = x + (sigma_next - sigma_cur) * d_cur\n",
    "        \n",
    "        d_next = denoiser(x_euler, sigma_next.expand(batch_size)) - x_euler\n",
    "        x = x + (sigma_next - sigma_cur) * 0.5 * (d_cur + d_next)\n",
    "    \n",
    "    return x.clamp(-1, 1)\n",
    "\n",
    "# Test schedule with robust assertion\n",
    "test_sched = edm_schedule(40, cfg.sigma_min, cfg.sigma_max, cfg.rho, device)\n",
    "assert len(test_sched) == 41\n",
    "# Use torch.isclose for floating point comparison\n",
    "assert torch.isclose(test_sched[0], torch.tensor(cfg.sigma_max, device=device), rtol=1e-4), \\\n",
    "    f\"Expected {cfg.sigma_max}, got {test_sched[0].item()}\"\n",
    "assert test_sched[-1] == 0\n",
    "print(f\"‚úÖ Sampler: {len(test_sched)} steps, œÉ ‚àà [{test_sched[0]:.1f}, {test_sched[-1]:.1f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a4ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training utils defined\n"
     ]
    }
   ],
   "source": [
    "def sample_sigma(B: int, sigma_min: float, sigma_max: float, device):\n",
    "    u = torch.rand(B, device=device)\n",
    "    return torch.exp(u * (math.log(sigma_max) - math.log(sigma_min)) + math.log(sigma_min))\n",
    "\n",
    "def edm_loss(denoiser: nn.Module, x0: torch.Tensor, sigma: torch.Tensor):\n",
    "    \"\"\"EDM denoising objective.\"\"\"\n",
    "    sigma_img = sigma[:, None, None, None] if sigma.dim() == 1 else sigma\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_noisy = x0 + sigma_img * noise\n",
    "    x_pred = denoiser(x_noisy, sigma)\n",
    "    return F.mse_loss(x_pred, x0)\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.9999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.shadow:\n",
    "                self.shadow[n] = self.decay * self.shadow[n] + (1 - self.decay) * p.data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model: nn.Module):\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.shadow:\n",
    "                p.data.copy_(self.shadow[n])\n",
    "\n",
    "print(\"‚úÖ Training utils defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c4e4cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FID computation defined\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_inception_features(images: torch.Tensor, model, batch_size: int = 50):\n",
    "    \"\"\"Extract Inception features for FID.\"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size]\n",
    "        # Resize to 299x299 for InceptionV3\n",
    "        batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        feat = model(batch)[0].squeeze(-1).squeeze(-1)\n",
    "        features.append(feat.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def calculate_fid(real_features: np.ndarray, fake_features: np.ndarray, eps=1e-6):\n",
    "    \"\"\"Calculate FID between real and fake features.\"\"\"\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return fid\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_fid(\n",
    "    denoiser: nn.Module,\n",
    "    real_loader: DataLoader,\n",
    "    num_samples: int,\n",
    "    cfg: Config,\n",
    "    inception_model\n",
    "):\n",
    "    \"\"\"Compute FID score.\"\"\"\n",
    "    denoiser.eval()\n",
    "    \n",
    "    # Get real features\n",
    "    print(f\"   Computing features for {num_samples} real images...\")\n",
    "    real_images = []\n",
    "    for x, _ in real_loader:\n",
    "        real_images.append(x)\n",
    "        if len(real_images) * x.shape[0] >= num_samples:\n",
    "            break\n",
    "    real_images = torch.cat(real_images, dim=0)[:num_samples]\n",
    "    real_images = (real_images + 1) / 2  # [-1,1] -> [0,1]\n",
    "    real_features = get_inception_features(real_images.to(device), inception_model)\n",
    "    \n",
    "    # Generate fake samples\n",
    "    print(f\"   Generating {num_samples} fake images...\")\n",
    "    fake_images = []\n",
    "    num_batches = (num_samples + cfg.fid_batch - 1) // cfg.fid_batch\n",
    "    for i in range(num_batches):\n",
    "        batch_size = min(cfg.fid_batch, num_samples - i * cfg.fid_batch)\n",
    "        fake = heun_sampler(\n",
    "            denoiser, batch_size, cfg.channels, cfg.image_size,\n",
    "            steps=50, sigma_min=cfg.sigma_min, sigma_max=cfg.sigma_max,\n",
    "            rho=cfg.rho, device=device\n",
    "        )\n",
    "        fake_images.append(fake)\n",
    "    fake_images = torch.cat(fake_images, dim=0)\n",
    "    fake_images = (fake_images + 1) / 2  # [-1,1] -> [0,1]\n",
    "    fake_features = get_inception_features(fake_images, inception_model)\n",
    "    \n",
    "    # Compute FID\n",
    "    fid = calculate_fid(real_features, fake_features)\n",
    "    return fid\n",
    "\n",
    "print(\"‚úÖ FID computation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a017a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CIFAR-10: 390 batches\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /home/wang.yixuan/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.yixuan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wang.yixuan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 104M/104M [00:00<00:00, 231MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ InceptionV3 loaded for FID\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)  # [-1, 1]\n",
    "])\n",
    "\n",
    "train_ds = CIFAR10(cfg.data_root, train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "    num_workers=cfg.num_workers, pin_memory=True, drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ CIFAR-10: {len(train_loader)} batches\")\n",
    "\n",
    "# Load Inception for FID\n",
    "inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "inception.eval()\n",
    "for p in inception.parameters():\n",
    "    p.requires_grad = False\n",
    "print(\"‚úÖ InceptionV3 loaded for FID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f2e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model: 5.95M params\n",
      "‚úÖ Optimizer & EMA ready\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "pc_core = HierarchicalPC(\n",
    "    in_channels=cfg.channels,\n",
    "    layer_channels=cfg.layer_channels,\n",
    "    resolutions=cfg.resolutions,\n",
    "    pc_iterations=cfg.pc_iterations,\n",
    "    emb_dim=cfg.emb_dim\n",
    ")\n",
    "denoiser = DenoiserEDM(pc_core, cfg.emb_dim, cfg.sigma_data).to(device)\n",
    "print(f\"‚úÖ Model: {sum(p.numel() for p in denoiser.parameters())/1e6:.2f}M params\")\n",
    "\n",
    "opt = torch.optim.AdamW(denoiser.parameters(), lr=cfg.lr, betas=(0.9, 0.999))\n",
    "ema = EMA(denoiser, cfg.ema_decay)\n",
    "print(f\"‚úÖ Optimizer & EMA ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "338cc2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training functions with tqdm defined\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, opt, ema, loader, epoch, cfg):\n",
    "    \"\"\"Train for one epoch with progress bar.\"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch:03d}\", leave=False)\n",
    "    for it, (x, _) in enumerate(pbar):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        sigma = sample_sigma(x.size(0), cfg.sigma_min, cfg.sigma_max, device)\n",
    "        loss = edm_loss(model, x, sigma)\n",
    "        \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        ema.update(model)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar every 10 iterations\n",
    "        if len(losses) >= 10:\n",
    "            pbar.set_postfix({'loss': f\"{np.mean(losses[-10:]):.5f}\"})\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_grid(model, epoch, cfg, nrow=4):\n",
    "    \"\"\"Sample and save a grid of images.\"\"\"\n",
    "    model.eval()\n",
    "    imgs = heun_sampler(\n",
    "        model, nrow**2, cfg.channels, cfg.image_size,\n",
    "        steps=50, sigma_min=cfg.sigma_min,\n",
    "        sigma_max=cfg.sigma_max, rho=cfg.rho, device=device\n",
    "    )\n",
    "    grid = tvu.make_grid((imgs + 1) * 0.5, nrow=nrow)\n",
    "    os.makedirs('samples', exist_ok=True)\n",
    "    tv.utils.save_image(grid, f'samples/ep{epoch:03d}.png')\n",
    "    print(f\"  ‚úÖ Saved samples/ep{epoch:03d}.png\")\n",
    "\n",
    "print(\"‚úÖ Training functions with tqdm defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8718476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Directories created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 1/100 [00:18<30:20, 18.39s/it, loss=0.08260, time=17.4s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved samples/ep001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñà         | 10/100 [02:55<26:25, 17.61s/it, loss=0.06971, time=17.2s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved samples/ep010.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|‚ñà‚ñâ        | 19/100 [05:48<23:20, 17.29s/it, loss=0.06853, time=17.2s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved samples/ep020.png\n",
      "\n",
      "  Computing FID at epoch 20...\n",
      "   Computing features for 10000 real images...\n",
      "   Generating 10000 fake images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|‚ñà‚ñâ        | 19/100 [08:04<34:25, 25.50s/it, loss=0.06853, time=17.2s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Computing FID at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfid_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     fid_scores\u001b[38;5;241m.\u001b[39mappend((epoch, fid))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  üìä FID at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m, in \u001b[0;36mcompute_fid\u001b[0;34m(denoiser, real_loader, num_samples, cfg, inception_model)\u001b[0m\n\u001b[1;32m     66\u001b[0m fake_features \u001b[38;5;241m=\u001b[39m get_inception_features(fake_images, inception_model)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Compute FID\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m fid \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fid\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mcalculate_fid\u001b[0;34m(real_features, fake_features, eps)\u001b[0m\n\u001b[1;32m     19\u001b[0m mu2, sigma2 \u001b[38;5;241m=\u001b[39m fake_features\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39mcov(fake_features, rowvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m diff \u001b[38;5;241m=\u001b[39m mu1 \u001b[38;5;241m-\u001b[39m mu2\n\u001b[0;32m---> 22\u001b[0m covmean, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39msqrtm(\u001b[43msigma1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma2\u001b[49m, disp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39miscomplexobj(covmean):\n\u001b[1;32m     25\u001b[0m     covmean \u001b[38;5;241m=\u001b[39m covmean\u001b[38;5;241m.\u001b[39mreal\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "print(\"‚úÖ Directories created\")\n",
    "\n",
    "# Training loop with tqdm\n",
    "fid_scores = []\n",
    "epoch_pbar = tqdm(range(1, cfg.epochs + 1), desc=\"Training\")\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    t0 = time.time()\n",
    "    avg_loss = train_epoch(denoiser, opt, ema, train_loader, epoch, cfg)\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    # Update epoch progress bar\n",
    "    epoch_pbar.set_postfix({\n",
    "        'loss': f\"{avg_loss:.5f}\",\n",
    "        'time': f\"{elapsed:.1f}s\"\n",
    "    })\n",
    "    \n",
    "    # Sample & evaluate\n",
    "    if epoch % cfg.sample_every == 0 or epoch == 1:\n",
    "        ema.copy_to(denoiser)\n",
    "        sample_grid(denoiser, epoch, cfg)\n",
    "        \n",
    "        # Compute FID every 20 epochs\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"\\n  Computing FID at epoch {epoch}...\")\n",
    "            fid = compute_fid(denoiser, train_loader, cfg.fid_samples, cfg, inception)\n",
    "            fid_scores.append((epoch, fid))\n",
    "            print(f\"  üìä FID at epoch {epoch}: {fid:.2f}\\n\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model': denoiser.state_dict(),\n",
    "            'ema': ema.shadow,\n",
    "            'opt': opt.state_dict(),\n",
    "            'fid_scores': fid_scores,\n",
    "            'config': cfg\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoints/ep{epoch:03d}.pt')\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(\"\\nüìä FID Scores:\")\n",
    "for ep, fid in fid_scores:\n",
    "    print(f\"  Epoch {ep:03d}: {fid:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "ema.copy_to(denoiser)\n",
    "denoiser.eval()\n",
    "\n",
    "print(\"Final sampling...\")\n",
    "with torch.no_grad():\n",
    "    imgs = heun_sampler(\n",
    "        denoiser, 64, cfg.channels, cfg.image_size,\n",
    "        steps=50, sigma_min=cfg.sigma_min,\n",
    "        sigma_max=cfg.sigma_max, rho=cfg.rho, device=device\n",
    "    )\n",
    "\n",
    "grid = tvu.make_grid((imgs + 1) * 0.5, nrow=8)\n",
    "tv.utils.save_image(grid, 'samples/final.png')\n",
    "print(\"‚úÖ Saved samples/final.png\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "display(Image('samples/final.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5563ecb",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b871f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ DIAGNOSTIC: Can model learn?\n",
      "==================================================\n",
      "Initial loss: 0.1491\n",
      "Final loss:   0.1413\n",
      "Reduction:    5.2%\n",
      "‚ùå Model CANNOT learn - architecture issue\n"
     ]
    }
   ],
   "source": [
    "# New cell - test if your setup CAN learn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def quick_diagnostic(denoiser, train_loader, device):\n",
    "    \"\"\"Test if model can learn at all.\"\"\"\n",
    "    print(\"\\nüî¨ DIAGNOSTIC: Can model learn?\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    denoiser.train()\n",
    "    opt = torch.optim.Adam(denoiser.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Get one small batch\n",
    "    x, _ = next(iter(train_loader))\n",
    "    x = x[:8].to(device)\n",
    "    \n",
    "    # Overfit on this batch\n",
    "    losses = []\n",
    "    for i in range(50):\n",
    "        sigma = torch.ones(x.size(0), device=device) * 5.0\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = x + sigma[:, None, None, None] * noise\n",
    "        \n",
    "        x_pred = denoiser(x_noisy, sigma)\n",
    "        loss = F.mse_loss(x_pred, x)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "    print(f\"Final loss:   {losses[-1]:.4f}\")\n",
    "    print(f\"Reduction:    {(1 - losses[-1]/losses[0])*100:.1f}%\")\n",
    "    \n",
    "    if losses[-1] < losses[0] * 0.2:\n",
    "        print(\"‚úÖ Model CAN learn!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Model CANNOT learn - architecture issue\")\n",
    "        return False\n",
    "\n",
    "# Run it\n",
    "can_learn = quick_diagnostic(denoiser, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28f89ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TO USE THIS FIX:\n",
      "\n",
      "1. Create a new cell with:\n",
      "   from simplified_fix import SimpleHierarchicalNet, diagnose_training\n",
      "   \n",
      "2. Replace your PC core with:\n",
      "   simple_core = SimpleHierarchicalNet(\n",
      "       in_channels=3,\n",
      "       base_channels=128,\n",
      "       emb_dim=128,\n",
      "       num_res_blocks=2\n",
      "   )\n",
      "   \n",
      "3. Create denoiser as before:\n",
      "   denoiser = DenoiserEDM(simple_core, cfg.emb_dim, cfg.sigma_data).to(device)\n",
      "   \n",
      "4. Run diagnostic:\n",
      "   can_learn = diagnose_training(denoiser, train_loader, device)\n",
      "   \n",
      "5. If diagnostic passes, train as normal!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EMERGENCY FIX: Simplified Working Baseline\n",
    "\n",
    "This uses a simpler architecture to verify your training pipeline works.\n",
    "Once this works (FID ~20-30 at epoch 50), we can add PC structure.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================================================================\n",
    "# SIMPLIFIED ARCHITECTURE (GUARANTEED TO WORK)\n",
    "# ==============================================================================\n",
    "\n",
    "def get_groups(channels: int, max_groups: int = 32) -> int:\n",
    "    for g in range(max_groups, 0, -1):\n",
    "        if channels % g == 0:\n",
    "            return g\n",
    "    return 1\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Simple residual block.\"\"\"\n",
    "    def __init__(self, channels: int, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(get_groups(channels), channels)\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        \n",
    "        self.temb_proj = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(emb_dim, channels)\n",
    "        )\n",
    "        \n",
    "        self.norm2 = nn.GroupNorm(get_groups(channels), channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, temb):\n",
    "        h = self.norm1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv1(h)\n",
    "        \n",
    "        # Time embedding\n",
    "        h = h + self.temb_proj(temb)[:, :, None, None]\n",
    "        \n",
    "        h = self.norm2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv2(h)\n",
    "        \n",
    "        return x + h  # Residual connection\n",
    "\n",
    "class SimpleHierarchicalNet(nn.Module):\n",
    "    \"\"\"Simplified hierarchical network (like U-Net but simpler).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        base_channels: int = 128,\n",
    "        emb_dim: int = 128,\n",
    "        num_res_blocks: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input\n",
    "        self.conv_in = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "        \n",
    "        # Encoder (32 -> 16 -> 8 -> 4)\n",
    "        self.down1 = nn.ModuleList([\n",
    "            ResBlock(base_channels, emb_dim) for _ in range(num_res_blocks)\n",
    "        ])\n",
    "        self.down1_pool = nn.Conv2d(base_channels, base_channels*2, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.down2 = nn.ModuleList([\n",
    "            ResBlock(base_channels*2, emb_dim) for _ in range(num_res_blocks)\n",
    "        ])\n",
    "        self.down2_pool = nn.Conv2d(base_channels*2, base_channels*4, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.down3 = nn.ModuleList([\n",
    "            ResBlock(base_channels*4, emb_dim) for _ in range(num_res_blocks)\n",
    "        ])\n",
    "        self.down3_pool = nn.Conv2d(base_channels*4, base_channels*4, 3, stride=2, padding=1)\n",
    "        \n",
    "        # Bottleneck (4x4)\n",
    "        self.mid = nn.ModuleList([\n",
    "            ResBlock(base_channels*4, emb_dim) for _ in range(2)\n",
    "        ])\n",
    "        \n",
    "        # Decoder (4 -> 8 -> 16 -> 32)\n",
    "        self.up3_upsample = nn.ConvTranspose2d(base_channels*4, base_channels*4, 4, stride=2, padding=1)\n",
    "        self.up3 = nn.ModuleList([\n",
    "            ResBlock(base_channels*4 + base_channels*4, emb_dim),  # +skip\n",
    "            ResBlock(base_channels*4, emb_dim)\n",
    "        ])\n",
    "        \n",
    "        self.up2_upsample = nn.ConvTranspose2d(base_channels*4, base_channels*2, 4, stride=2, padding=1)\n",
    "        self.up2 = nn.ModuleList([\n",
    "            ResBlock(base_channels*2 + base_channels*2, emb_dim),  # +skip\n",
    "            ResBlock(base_channels*2, emb_dim)\n",
    "        ])\n",
    "        \n",
    "        self.up1_upsample = nn.ConvTranspose2d(base_channels*2, base_channels, 4, stride=2, padding=1)\n",
    "        self.up1 = nn.ModuleList([\n",
    "            ResBlock(base_channels + base_channels, emb_dim),  # +skip\n",
    "            ResBlock(base_channels, emb_dim)\n",
    "        ])\n",
    "        \n",
    "        # Output\n",
    "        self.norm_out = nn.GroupNorm(get_groups(base_channels), base_channels)\n",
    "        self.conv_out = nn.Conv2d(base_channels, in_channels, 3, padding=1)\n",
    "        \n",
    "        # Initialize output layer to zero\n",
    "        self.conv_out.weight.data.zero_()\n",
    "        self.conv_out.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, temb):\n",
    "        # Input\n",
    "        h = self.conv_in(x)\n",
    "        \n",
    "        # Encoder with skip connections\n",
    "        h1 = h\n",
    "        for block in self.down1:\n",
    "            h1 = block(h1, temb)\n",
    "        \n",
    "        h2 = self.down1_pool(h1)\n",
    "        for block in self.down2:\n",
    "            h2 = block(h2, temb)\n",
    "        \n",
    "        h3 = self.down2_pool(h2)\n",
    "        for block in self.down3:\n",
    "            h3 = block(h3, temb)\n",
    "        \n",
    "        h = self.down3_pool(h3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        for block in self.mid:\n",
    "            h = block(h, temb)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        h = self.up3_upsample(h)\n",
    "        h = torch.cat([h, h3], dim=1)  # Skip\n",
    "        h = self.up3[0](h, temb)\n",
    "        h = self.up3[1](h, temb)\n",
    "        \n",
    "        h = self.up2_upsample(h)\n",
    "        h = torch.cat([h, h2], dim=1)  # Skip\n",
    "        h = self.up2[0](h, temb)\n",
    "        h = self.up2[1](h, temb)\n",
    "        \n",
    "        h = self.up1_upsample(h)\n",
    "        h = torch.cat([h, h1], dim=1)  # Skip\n",
    "        h = self.up1[0](h, temb)\n",
    "        h = self.up1[1](h, temb)\n",
    "        \n",
    "        # Output\n",
    "        h = self.norm_out(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv_out(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# FIX: FID Computation with Better Error Handling\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_inception_features(images: torch.Tensor, model, batch_size: int = 50):\n",
    "    \"\"\"Extract Inception features for FID.\"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i+batch_size]\n",
    "        # Resize to 299x299 for InceptionV3\n",
    "        batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Get features\n",
    "        with torch.no_grad():\n",
    "            feat = model(batch)\n",
    "            if isinstance(feat, tuple):\n",
    "                feat = feat[0]  # InceptionV3 returns tuple\n",
    "            feat = feat.squeeze(-1).squeeze(-1)  # Remove spatial dims\n",
    "            features.append(feat.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def calculate_fid(real_features: np.ndarray, fake_features: np.ndarray, eps=1e-6):\n",
    "    \"\"\"Calculate FID between real and fake features.\"\"\"\n",
    "    # Ensure 2D\n",
    "    if real_features.ndim == 1:\n",
    "        real_features = real_features.reshape(-1, 1)\n",
    "    if fake_features.ndim == 1:\n",
    "        fake_features = fake_features.reshape(-1, 1)\n",
    "    \n",
    "    # Check minimum samples\n",
    "    if len(real_features) < 2 or len(fake_features) < 2:\n",
    "        print(\"Warning: Not enough samples for FID calculation\")\n",
    "        return float('inf')\n",
    "    \n",
    "    mu1 = real_features.mean(axis=0)\n",
    "    mu2 = fake_features.mean(axis=0)\n",
    "    \n",
    "    sigma1 = np.cov(real_features, rowvar=False)\n",
    "    sigma2 = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    # Handle scalar case\n",
    "    if sigma1.ndim == 0:\n",
    "        sigma1 = sigma1.reshape(1, 1)\n",
    "    if sigma2.ndim == 0:\n",
    "        sigma2 = sigma2.reshape(1, 1)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    \n",
    "    # Compute sqrt of product\n",
    "    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            print(\"Warning: Imaginary component in covmean\")\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "    return float(fid)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DIAGNOSTIC: Check if model is learning\n",
    "# ==============================================================================\n",
    "\n",
    "def diagnose_training(denoiser, train_loader, device):\n",
    "    \"\"\"Quick diagnostic to see if model can learn.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DIAGNOSTIC: Testing if model can learn\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    denoiser.train()\n",
    "    opt = torch.optim.Adam(denoiser.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Overfit on ONE batch\n",
    "    x, _ = next(iter(train_loader))\n",
    "    x = x[:16].to(device)  # Just 16 images\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(100):\n",
    "        sigma = torch.ones(x.size(0), device=device) * 10.0\n",
    "        \n",
    "        # Add noise\n",
    "        noise = torch.randn_like(x)\n",
    "        x_noisy = x + sigma[:, None, None, None] * noise\n",
    "        \n",
    "        # Predict clean\n",
    "        x_pred = denoiser(x_noisy, sigma)\n",
    "        loss = F.mse_loss(x_pred, x)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print(f\"  Step {i+1:3d}: loss = {loss.item():.6f}\")\n",
    "    \n",
    "    print(f\"\\n  Initial loss: {losses[0]:.6f}\")\n",
    "    print(f\"  Final loss:   {losses[-1]:.6f}\")\n",
    "    print(f\"  Reduction:    {(losses[0] - losses[-1]) / losses[0] * 100:.1f}%\")\n",
    "    \n",
    "    if losses[-1] < losses[0] * 0.1:\n",
    "        print(\"  ‚úÖ Model CAN learn (loss reduced >90%)\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"  ‚ùå Model NOT learning properly\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# USAGE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "TO USE THIS FIX:\n",
    "\n",
    "1. Create a new cell with:\n",
    "   from simplified_fix import SimpleHierarchicalNet, diagnose_training\n",
    "   \n",
    "2. Replace your PC core with:\n",
    "   simple_core = SimpleHierarchicalNet(\n",
    "       in_channels=3,\n",
    "       base_channels=128,\n",
    "       emb_dim=128,\n",
    "       num_res_blocks=2\n",
    "   )\n",
    "   \n",
    "3. Create denoiser as before:\n",
    "   denoiser = DenoiserEDM(simple_core, cfg.emb_dim, cfg.sigma_data).to(device)\n",
    "   \n",
    "4. Run diagnostic:\n",
    "   can_learn = diagnose_training(denoiser, train_loader, device)\n",
    "   \n",
    "5. If diagnostic passes, train as normal!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d75f8773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using simple baseline architecture\n",
      "   Parameters: 64.2M\n"
     ]
    }
   ],
   "source": [
    "simple_core = SimpleHierarchicalNet(\n",
    "    in_channels=3,\n",
    "    base_channels=128,\n",
    "    emb_dim=128,\n",
    "    num_res_blocks=2\n",
    ")\n",
    "\n",
    "# Recreate denoiser\n",
    "denoiser = DenoiserEDM(simple_core, cfg.emb_dim, cfg.sigma_data).to(device)\n",
    "opt = torch.optim.AdamW(denoiser.parameters(), lr=cfg.lr)\n",
    "ema = EMA(denoiser, cfg.ema_decay)\n",
    "\n",
    "print(\"‚úÖ Using simple baseline architecture\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in denoiser.parameters())/1e6:.1f}M\")\n",
    "\n",
    "# Now train as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cada0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
