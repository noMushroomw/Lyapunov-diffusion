{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240d3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torch_fidelity lpips --quiet\n",
    "import math, random, os, functools, itertools, time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils as vutils\n",
    "\n",
    "# Optional: LPIPS for RF pre-metric (installed above)\n",
    "try:\n",
    "    import lpips\n",
    "    LPIPS_AVAILABLE = True\n",
    "except Exception:\n",
    "    LPIPS_AVAILABLE = False\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d49fa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Data\n",
    "    dataset: str = 'CIFAR10'  # 'CIFAR10' or your dataset name\n",
    "    data_root: str = './data'\n",
    "    image_size: int = 32\n",
    "    channels: int = 3\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 128\n",
    "    num_steps: int = 300_000\n",
    "    lr: float = 2e-4\n",
    "    ema_decay: float = 0.999  # EMA for sampling stability\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Objective toggle\n",
    "    objective: str = 'edm'  # 'edm' or 'flow' ('flow' = flow/rectified-flow matching)\n",
    "\n",
    "    # EDM noise schedule (Table 1, EDM)\n",
    "    sigma_min: float = 0.002\n",
    "    sigma_max: float = 80.0\n",
    "    sigma_data: float = 0.5\n",
    "    rho: float = 7.0\n",
    "    P_mean: float = -1.2  # log-sigma mean\n",
    "    P_std: float = 1.2    # log-sigma std\n",
    "\n",
    "    # Sampler\n",
    "    NFE: int = 35  # number of function evaluations (Heun steps)\n",
    "    sample_bs: int = 64\n",
    "    save_dir: str = './samples'\n",
    "\n",
    "cfg = Config()\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94882c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(cfg: Config):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize(cfg.image_size),\n",
    "        transforms.CenterCrop(cfg.image_size),\n",
    "        transforms.ToTensor(),  # [0,1]\n",
    "        # map to [-1,1] for EDM / standard diffusion\n",
    "        transforms.Lambda(lambda x: x * 2.0 - 1.0),\n",
    "    ])\n",
    "    if cfg.dataset.upper() == 'CIFAR10':\n",
    "        trainset = datasets.CIFAR10(root=cfg.data_root, train=True, download=True, transform=tfm)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Add your dataset here.\")\n",
    "    trainloader = DataLoader(trainset, batch_size=cfg.batch_size, shuffle=True,\n",
    "                             drop_last=True, num_workers=4, pin_memory=True)\n",
    "    return trainloader\n",
    "\n",
    "trainloader = make_dataloaders(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6ee8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        t is (B,) of log-sigma (EDM) or time t in [0,1] (Flow). We still feed a scalar with sin-cos.\n",
    "        \"\"\"\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(torch.arange(half, device=t.device) * (-math.log(10_000) / max(half - 1, 1)))\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        if self.dim % 2 == 1:  # pad if odd\n",
    "            emb = F.pad(emb, (0,1))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c947c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, groups=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.GroupNorm(groups, in_ch)\n",
    "        self.act = nn.SiLU()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(tdim, out_ch)\n",
    "        )\n",
    "        self.norm2 = nn.GroupNorm(groups, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.skip = (nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity())\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.conv1(self.act(self.norm1(x)))\n",
    "        h = h + self.emb(temb)[:, :, None, None]\n",
    "        h = self.conv2(self.act(self.norm2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, ch, heads=4):\n",
    "        super().__init__()\n",
    "        self.norm = nn.GroupNorm(8, ch)\n",
    "        self.qkv = nn.Conv2d(ch, ch*3, 1)\n",
    "        self.proj = nn.Conv2d(ch, ch, 1)\n",
    "        self.heads = heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        q,k,v = qkv.chunk(3, dim=1)\n",
    "        q = q.view(b, self.heads, c//self.heads, h*w)\n",
    "        k = k.view(b, self.heads, c//self.heads, h*w)\n",
    "        v = v.view(b, self.heads, c//self.heads, h*w)\n",
    "        attn = torch.softmax((q.transpose(-2,-1) @ k) / math.sqrt(c//self.heads), dim=-1)  # (B,H,HW,HW)\n",
    "        out = (attn @ v.transpose(-2,-1)).transpose(-2,-1).contiguous()\n",
    "        out = out.view(b, c, h, w)\n",
    "        return x + self.proj(out)\n",
    "\n",
    "class EDMUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet backbone. Outputs raw network F_theta; EDM preconditioning is applied outside.\n",
    "    Deep supervision: intermediate x0 heads to realize 'predictive coding' residual learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: Config, ch=128, ch_mult=(1,2,2,2), num_res=2, attn_res=(16,)):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.in_conv = nn.Conv2d(cfg.channels, ch, 3, padding=1)\n",
    "        tdim = ch * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalTimeEmbedding(tdim),\n",
    "            nn.Linear(tdim, tdim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(tdim, tdim),\n",
    "        )\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.skip_channels = []\n",
    "\n",
    "        chs = [ch]\n",
    "        curr_ch = ch\n",
    "        # Down path\n",
    "        in_size = cfg.image_size\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res):\n",
    "                self.downs.append(ResBlock(curr_ch, out_ch, tdim))\n",
    "                curr_ch = out_ch\n",
    "                if in_size in attn_res:\n",
    "                    self.downs.append(AttentionBlock(curr_ch))\n",
    "            self.downs.append(nn.Conv2d(curr_ch, curr_ch, 3, stride=2, padding=1))\n",
    "            chs.append(curr_ch)\n",
    "            in_size //= 2\n",
    "\n",
    "        # Middle\n",
    "        self.mid = nn.Sequential(\n",
    "            ResBlock(curr_ch, curr_ch, tdim),\n",
    "            AttentionBlock(curr_ch),\n",
    "            ResBlock(curr_ch, curr_ch, tdim),\n",
    "        )\n",
    "\n",
    "        # Up path\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res+1):\n",
    "                self.ups.append(ResBlock(curr_ch + chs.pop(), out_ch, tdim))\n",
    "                curr_ch = out_ch\n",
    "                if in_size in attn_res:\n",
    "                    self.ups.append(AttentionBlock(curr_ch))\n",
    "            self.ups.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            in_size *= 2\n",
    "\n",
    "        self.out_norm = nn.GroupNorm(8, curr_ch)\n",
    "        self.out = nn.Conv2d(curr_ch, cfg.channels, 3, padding=1)\n",
    "\n",
    "        # Deep supervision heads for 'predictive coding' (optional)\n",
    "        self.aux_heads = nn.ModuleList([\n",
    "            nn.Conv2d(ch * mult, cfg.channels, 1) for mult in ch_mult\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, t_scalar):\n",
    "        # t_scalar is log_sigma (EDM) or time t in [0,1] (Flow). Embed:\n",
    "        temb = self.time_mlp[0](t_scalar)  # sinusoidal\n",
    "        temb = self.time_mlp[1:](temb)\n",
    "\n",
    "        hs = []\n",
    "        h = self.in_conv(x)\n",
    "        # Down\n",
    "        aux_preds = []\n",
    "        attn_resolutions = set([16])\n",
    "        size = x.shape[-1]\n",
    "        for m in self.downs:\n",
    "            if isinstance(m, ResBlock):\n",
    "                h = m(h, temb)\n",
    "            else:\n",
    "                h = m(h)\n",
    "            if isinstance(m, ResBlock):\n",
    "                hs.append(h)\n",
    "        # Middle\n",
    "        h = self.mid(h, temb)\n",
    "\n",
    "        # Up\n",
    "        up_idx = 0\n",
    "        for m in self.ups:\n",
    "            if isinstance(m, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "                h = m(h, temb)\n",
    "            elif isinstance(m, AttentionBlock):\n",
    "                h = m(h)\n",
    "            else:\n",
    "                h = m(h)\n",
    "                # Each time we upsample, produce an aux head on this scale if available\n",
    "                if up_idx < len(self.aux_heads):\n",
    "                    aux_preds.append(self.aux_heads[up_idx](h))\n",
    "                    up_idx += 1\n",
    "\n",
    "        h = self.out(self.out_norm(h))\n",
    "        return h, aux_preds  # raw network output F_theta (no preconditioning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0177a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_log_normal_sigma(bs, P_mean, P_std, device):\n",
    "    # Sample log sigma ~ N(P_mean, P_std^2)\n",
    "    return torch.exp(P_mean + P_std * torch.randn(bs, device=device))\n",
    "\n",
    "def karras_sigma_schedule(N, sigma_min, sigma_max, rho=7.0, device='cpu'):\n",
    "    # Monotone decreasing σ sequence used for sampling; see EDM Table 1 & Alg 1\n",
    "    ramp = torch.linspace(0, 1, N, device=device)\n",
    "    min_inv_rho = sigma_min**(1/rho)\n",
    "    max_inv_rho = sigma_max**(1/rho)\n",
    "    sigmas = (max_inv_rho + ramp * (min_inv_rho - max_inv_rho))**rho\n",
    "    return sigmas\n",
    "\n",
    "class EDMPrecondWrapper(nn.Module):\n",
    "    def __init__(self, net: EDMUNet, sigma_data: float):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.sigma_data = sigma_data\n",
    "\n",
    "    def forward(self, x, sigma):\n",
    "        \"\"\"\n",
    "        x: (B,C,H,W) in [-1,1]\n",
    "        sigma: (B,) positive noise level\n",
    "        returns: denoised x0 estimate (EDM preconditioned output)\n",
    "        \"\"\"\n",
    "        sigma = sigma.view(-1, 1, 1, 1)\n",
    "        c_skip = (self.sigma_data**2) / (sigma**2 + self.sigma_data**2)\n",
    "        c_out  = (sigma * self.sigma_data) / torch.sqrt(sigma**2 + self.sigma_data**2)\n",
    "        c_in   = 1.0 / torch.sqrt(sigma**2 + self.sigma_data**2)\n",
    "        # c_noise is log-sigma / 4 per EDM\n",
    "        c_noise = 0.25 * torch.log(sigma.squeeze(-1).squeeze(-1).squeeze(-1))\n",
    "\n",
    "        f_raw, aux_preds = self.net(c_in * x, c_noise)\n",
    "        D = c_skip * x + c_out * f_raw\n",
    "        return D, aux_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "469e03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_xt_rf(x0, eps, t):\n",
    "    # xt = (1-t) x0 + t eps ; eps ~ N(0,I), t in [0,1]\n",
    "    return (1.0 - t)[:, None, None, None] * x0 + t[:, None, None, None] * eps\n",
    "\n",
    "def flow_matching_targets(x0, eps):\n",
    "    # v = d/dt xt = eps - x0  (constant in t for linear path)\n",
    "    return eps - x0\n",
    "\n",
    "def u_shaped_t(bs, device):\n",
    "    # U-shaped distribution (more mass near 0 and 1), improves few-step performance\n",
    "    u = torch.rand(bs, device=device)\n",
    "    t = 0.5 - 0.5 * torch.cos(math.pi * u)\n",
    "    return t.clamp(1e-5, 1-1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b56299",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     opt \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, ema, opt\n\u001b[0;32m---> 11\u001b[0m model, ema, opt \u001b[38;5;241m=\u001b[39m \u001b[43mmake_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m global_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_ema\u001b[39m(ema, model, decay):\n",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m, in \u001b[0;36mmake_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake_model\u001b[39m(cfg: Config):\n\u001b[0;32m----> 2\u001b[0m     net \u001b[38;5;241m=\u001b[39m \u001b[43mEDMUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m EDMPrecondWrapper(net, sigma_data\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msigma_data)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      4\u001b[0m     ema \u001b[38;5;241m=\u001b[39m EDMPrecondWrapper(EDMUNet(cfg), sigma_data\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39msigma_data)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "Cell \u001b[0;32mIn[7], line 87\u001b[0m, in \u001b[0;36mEDMUNet.__init__\u001b[0;34m(self, cfg, ch, ch_mult, num_res, attn_res)\u001b[0m\n\u001b[1;32m     85\u001b[0m out_ch \u001b[38;5;241m=\u001b[39m ch \u001b[38;5;241m*\u001b[39m mult\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_res\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mups\u001b[38;5;241m.\u001b[39mappend(ResBlock(curr_ch \u001b[38;5;241m+\u001b[39m \u001b[43mchs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, out_ch, tdim))\n\u001b[1;32m     88\u001b[0m     curr_ch \u001b[38;5;241m=\u001b[39m out_ch\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m in_size \u001b[38;5;129;01min\u001b[39;00m attn_res:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "def make_model(cfg: Config):\n",
    "    net = EDMUNet(cfg)\n",
    "    model = EDMPrecondWrapper(net, sigma_data=cfg.sigma_data).to(DEVICE)\n",
    "    ema = EDMPrecondWrapper(EDMUNet(cfg), sigma_data=cfg.sigma_data).to(DEVICE)\n",
    "    ema.load_state_dict(model.state_dict())\n",
    "    for p in ema.parameters():\n",
    "        p.requires_grad = False\n",
    "    opt = AdamW(model.parameters(), lr=cfg.lr, weight_decay=0.0, betas=(0.9, 0.999))\n",
    "    return model, ema, opt\n",
    "\n",
    "model, ema, opt = make_model(cfg)\n",
    "global_step = 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(ema, model, decay):\n",
    "    msd = model.state_dict()\n",
    "    for k, v in ema.state_dict().items():\n",
    "        if v.dtype.is_floating_point:\n",
    "            ema.state_dict()[k].copy_(decay * v + (1.0 - decay) * msd[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7788e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wang.yixuan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wang.yixuan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /blue/wdixon/wang.yixuan/.conda/envs/ly/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "lpips_fn = lpips.LPIPS(net='vgg').to(DEVICE).eval() if LPIPS_AVAILABLE else None\n",
    "\n",
    "def loss_lpips_huber(x_hat, x0, delta=0.01):\n",
    "    # LPIPS + Huber (optional, RF low-NFE improvements)\n",
    "    loss = 0.0\n",
    "    if LPIPS_AVAILABLE:\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "        p = lpips_fn(torch.clamp((x_hat+1)/2,0,1), torch.clamp((x0+1)/2,0,1))\n",
    "        loss = p.mean()\n",
    "    # small Huber in pixel space\n",
    "    diff = x_hat - x0\n",
    "    huber = torch.where(diff.abs() < delta, 0.5*diff**2, delta*(diff.abs()-0.5*delta)).mean()\n",
    "    return loss + huber\n",
    "\n",
    "def train_step(batch, cfg: Config, model, ema, opt, objective='edm'):\n",
    "    global global_step\n",
    "    x0 = batch[0].to(DEVICE)  # in [-1,1]\n",
    "    B = x0.shape[0]\n",
    "\n",
    "    if objective == 'edm':\n",
    "        # Sample σ ~ LogNormal(P_mean, P_std)\n",
    "        sigma = rand_log_normal_sigma(B, cfg.P_mean, cfg.P_std, DEVICE).clamp(cfg.sigma_min, cfg.sigma_max)\n",
    "        n = torch.randn_like(x0)\n",
    "        x_noisy = x0 + sigma[:, None, None, None] * n\n",
    "        # D_theta preconditioned output (x0 prediction)\n",
    "        xhat, aux_preds = model(x_noisy, sigma)\n",
    "        # EDM weighting\n",
    "        lam = (sigma**2 + cfg.sigma_data**2) / ((sigma * cfg.sigma_data)**2)\n",
    "        pix_loss = (lam[:, None, None, None] * (xhat - x0)**2).mean()\n",
    "\n",
    "        # Deep supervision (predictive-coding heads): small weight\n",
    "        aux_loss = 0.0\n",
    "        for aux in aux_preds:\n",
    "            # predict x0 at intermediate scales\n",
    "            aux_loss += F.mse_loss(aux, F.interpolate(x0, aux.shape[-2:], mode='bilinear', align_corners=False))\n",
    "        aux_loss = 0.25 * aux_loss / max(1, len(aux_preds))\n",
    "\n",
    "        loss = pix_loss + aux_loss\n",
    "\n",
    "    elif objective == 'flow':\n",
    "        # Rectified Flow / Flow Matching loss:\n",
    "        t = u_shaped_t(B, DEVICE)\n",
    "        eps = torch.randn_like(x0)\n",
    "        xt = sample_xt_rf(x0, eps, t)\n",
    "        # Use log-sigma embedding channel but pass time t -> c_noise := t in [-?], we map to [-5,5]\n",
    "        # reuse preconditioner wrapper's call (it expects sigma); we pass fake sigma via mapping t->exp(4t-2)\n",
    "        # Simpler: call raw net with time-embedding of t (override wrapper)\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "        # call underlying UNet directly\n",
    "        raw_net = model.net\n",
    "        f_raw, aux_preds = raw_net(xt, t)  # raw predicts \"velocity\" proxy\n",
    "        v_target = flow_matching_targets(x0, eps)\n",
    "        v_pred = f_raw  # treat as velocity\n",
    "        # L2 or LPIPS-Huber-augmented\n",
    "        loss_main = F.mse_loss(v_pred, v_target)\n",
    "        if LPIPS_AVAILABLE:\n",
    "            # optional perceptual anchoring of x0 estimates from a one-step integration guess\n",
    "            xhat = xt + (v_pred)[:, :, :, :] * (1.0 - t)[:, None, None, None]\n",
    "            loss_main = loss_main + 0.05 * loss_lpips_huber(xhat, x0)\n",
    "\n",
    "        # Deep supervision (aux heads try to predict downsampled velocity target)\n",
    "        aux_loss = 0.0\n",
    "        for aux in aux_preds:\n",
    "            v_down = F.interpolate(v_target, aux.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            aux_loss += F.mse_loss(aux, v_down)\n",
    "        aux_loss = 0.25 * aux_loss / max(1, len(aux_preds))\n",
    "\n",
    "        loss = loss_main + aux_loss\n",
    "    else:\n",
    "        raise ValueError(\"objective must be 'edm' or 'flow'\")\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    if cfg.grad_clip is not None:\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "    opt.step()\n",
    "    update_ema(ema, model, cfg.ema_decay)\n",
    "\n",
    "    global_step += 1\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba196ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def heun_sampler_ema(ema, cfg: Config, num: int = 64):\n",
    "    ema.eval()\n",
    "    sigmas = karras_sigma_schedule(cfg.NFE, cfg.sigma_min, cfg.sigma_max, cfg.rho, device=DEVICE)\n",
    "    x = torch.randn(num, cfg.channels, cfg.image_size, cfg.image_size, device=DEVICE) * sigmas[0]\n",
    "    for i in range(len(sigmas)-1):\n",
    "        sigma_i = sigmas[i]\n",
    "        sigma_j = sigmas[i+1]\n",
    "        # Derivative of x w.r.t. t (prob flow ODE form): dx/dt = -(σ̇/σ)[ x - D(x;σ) ]\n",
    "        # In discrete form with σ as the \"time\" parameterizing, EDM uses Heun (improved Euler):\n",
    "        #   d_i = ( (sigma_dot/sigma) * x - (sigma_dot) * D(x; sigma)/sigma )  -> implemented via EDM Alg 1\n",
    "        # We use the simpler practical Euler predictor + trapezoid corrector based on D(x;σ).\n",
    "        # (Exact EDM code uses the same structure; see Alg. 1 lines 4–8; guard at σ->0) :contentReference[oaicite:11]{index=11}\n",
    "\n",
    "        # Compute derivative at i\n",
    "        D_i, _ = ema(x, torch.full((num,), sigma_i, device=DEVICE))\n",
    "        d_i = (D_i - x) / (sigma_i**2) * (- (sigma_j - sigma_i)) * (sigma_i)  # scaled step; equivalent form\n",
    "        # Euler step\n",
    "        x_euler = x + d_i\n",
    "\n",
    "        if sigma_j > 0:\n",
    "            # 2nd-order correction (Heun/trapezoid)\n",
    "            D_j, _ = ema(x_euler, torch.full((num,), sigma_j, device=DEVICE))\n",
    "            d_j = (D_j - x_euler) / (sigma_j**2) * (- (sigma_j - sigma_i)) * (sigma_j)\n",
    "            x = x + 0.5 * (d_i + d_j)\n",
    "        else:\n",
    "            # At σ=0, use Euler (avoid division by zero)\n",
    "            x = x_euler\n",
    "\n",
    "    x = x.clamp(-1, 1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91f477d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m             grid \u001b[38;5;241m=\u001b[39m vutils\u001b[38;5;241m.\u001b[39mmake_grid((samples\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39msqrt(cfg\u001b[38;5;241m.\u001b[39msample_bs)))\n\u001b[1;32m     17\u001b[0m             vutils\u001b[38;5;241m.\u001b[39msave_image(grid, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39msave_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m train(cfg, \u001b[43mmodel\u001b[49m, ema, opt, trainloader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def train(cfg: Config, model, ema, opt, trainloader):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "    for it, batch in enumerate(itertools.islice(itertools.cycle(trainloader), cfg.num_steps)):\n",
    "        loss = train_step(batch, cfg, model, ema, opt, objective=cfg.objective)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if (it+1) % 100 == 0:\n",
    "            print(f\"[{it+1}/{cfg.num_steps}] loss={sum(losses[-100:])/100:.4f}  time/100={time.time()-t0:.1f}s\")\n",
    "            t0 = time.time()\n",
    "\n",
    "        if (it+1) % 5000 == 0:\n",
    "            with torch.no_grad():\n",
    "                samples = heun_sampler_ema(ema, cfg, num=cfg.sample_bs)\n",
    "            grid = vutils.make_grid((samples+1)/2, nrow=int(math.sqrt(cfg.sample_bs)))\n",
    "            vutils.save_image(grid, os.path.join(cfg.save_dir, f\"sample_{cfg.objective}_{it+1}.png\"))\n",
    "\n",
    "train(cfg, model, ema, opt, trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training (or loading a checkpoint), sample:\n",
    "with torch.no_grad():\n",
    "    samples = heun_sampler_ema(ema, cfg, num=cfg.sample_bs)\n",
    "grid = vutils.make_grid((samples+1)/2, nrow=int(math.sqrt(cfg.sample_bs)))\n",
    "vutils.save_image(grid, os.path.join(cfg.save_dir, f\"final_{cfg.objective}.png\"))\n",
    "grid\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
